% IEEE Transactions on Knowledge and Data Engineering Paper
% EGIS: Evolutionary Grammar for Interpretable Streams
% Explainable Rule-Based Classification for Data Streams with Concept Drift

\documentclass[lettersize,journal]{IEEEtran}

% Packages (IEEE Computer Society template)
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{url}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{pdflscape}
\usepackage{rotating}
\usepackage{float}

% Advanced typography and hyphenation control
\usepackage[protrusion=true,expansion=true,factor=1100]{microtype}

% Hyphenation penalties - strongly discourage hyphenation
\hyphenpenalty=5000
\exhyphenpenalty=5000

% Text justification parameters - allow more flexible spacing
\tolerance=3000
\emergencystretch=10pt

% Prevent hyphenation of specific technical terms
\hyphenation{
  EGIS
  interpretable
  streams
  concept
  drift
  grammatical
  evolution
  evolutionary
  classification
  adaptation
  transition
  metrics
}

% Custom commands
\newcommand{\egis}{EGIS}
\newcommand{\gmean}{G-Mean}

\begin{document}

\title{An Explainable Evolutionary Grammar Approach for Interpretable Data Stream Classification with Concept Drift}

\author{Leandro Maciel Almeida~and~Leandro L. Minku,~\IEEEmembership{Senior Member,~IEEE}
\thanks{L. M. Almeida is with the Centro de Inform\'{a}tica, Universidade Federal de Pernambuco, Recife, Brazil (e-mail: lma3@cin.ufpe.br).}
\thanks{L. L. Minku is with the School of Computer Science, University of Birmingham, Birmingham, B15 2TT, UK.}}

\markboth{IEEE Transactions on Knowledge and Data Engineering}%
{Almeida and Minku: An Explainable Evolutionary Grammar Approach for Data Stream Classification}

\maketitle

\begin{abstract}
Data stream classification under concept drift presents fundamental challenges for machine learning systems, requiring models that adapt to evolving data distributions while maintaining interpretability. This paper introduces a novel rule-based classifier, termed EGIS, that leverages grammatical evolution to generate human-readable classification rules for non-stationary data streams. Unlike black-box ensemble methods that sacrifice transparency for accuracy, EGIS produces explicit IF-THEN rules that enable domain experts to understand, validate, and trust the model's predictions. The proposed method incorporates a multi-level self-adaptation framework that automatically adjusts evolutionary parameters in response to population diversity, performance history, and drift severity signals. A specialized mutation mechanism, termed Gene Therapy, injects discriminative patterns extracted from auxiliary decision trees directly into the evolutionary population, accelerating convergence during concept drift recovery. We introduce three novel transition metrics, namely Transition Change Score (TCS), Rule Instability Rate (RIR), and Average Modification Severity (AMS), that quantify rule evolution dynamics, revealing not only what the classifier predicts but how and why its decision logic changes over time. Comprehensive experiments across 48 datasets encompassing abrupt, gradual, recurring, and noisy drift scenarios demonstrate that EGIS achieves competitive predictive performance (average G-Mean of 0.78) while providing complete interpretability. Statistical analysis confirms that EGIS significantly outperforms ERulesD2S by 22-24 percentage points while maintaining a gap of only 2-4\% relative to black-box ensemble methods. The proposed transition metrics provide unprecedented insight into classifier adaptation behavior, enabling practitioners to monitor how classification rules evolve in response to changing data distributions.
\end{abstract}

\begin{IEEEkeywords}
Data stream classification, concept drift, explainable AI, grammatical evolution, rule-based learning, interpretable machine learning, self-adaptation
\end{IEEEkeywords}

%==============================================================================
\section{Introduction}
\label{sec:introduction}
%==============================================================================

The proliferation of real-time data sources across diverse domains, ranging from financial transaction monitoring to industrial sensor networks, has created an urgent need for classification systems capable of processing continuous data streams while adapting to evolving patterns~\cite{gama2014survey}. Unlike traditional batch learning paradigms where a model is trained once on a static dataset, data stream classification must address a constellation of interrelated challenges: concept drift manifests as changes in the underlying data distribution that render previously learned models obsolete; computational constraints demand single-pass processing with bounded memory; and the requirement for timely predictions precludes the luxury of storing data for retrospective analysis~\cite{bifet2010moa}. These challenges have motivated extensive research into adaptive learning algorithms that can detect distribution changes and update their models accordingly.

While the machine learning community has made substantial progress in developing adaptive classifiers for non-stationary environments, the dominant approaches rely on ensemble methods or complex tree structures that achieve high predictive accuracy at the cost of interpretability~\cite{gomes2017adaptive}. Methods such as Adaptive Random Forest (ARF)~\cite{gomes2017adaptive} and Streaming Random Patches (SRP)~\cite{gomes2019streaming} combine hundreds of base learners whose individual predictions are aggregated through mechanisms that obscure the reasoning process from human understanding. This opacity is particularly problematic in high-stakes domains such as healthcare diagnostics, financial fraud detection, and critical infrastructure monitoring, where understanding \textit{why} a model makes specific predictions is as important as the predictions themselves~\cite{rudin2019stop}. Domain experts in these fields cannot validate or trust a classifier whose decision logic remains hidden within an impenetrable ensemble, regardless of its accuracy on benchmark datasets.

The tension between predictive performance and interpretability represents a fundamental challenge in machine learning, yet the data streaming context introduces an additional dimension that has received insufficient attention: the need to understand not only \textit{what} a model predicts at any given moment, but \textit{how} and \textit{why} its decision logic changes over time. When concept drift occurs, an adaptive classifier must modify its internal representation to track the evolving data distribution. For black-box methods, this adaptation process is entirely opaque; practitioners observe only that predictions have changed, with no insight into which aspects of the model were modified or why. In contrast, an interpretable classifier that maintains explicit decision rules offers the potential for transparency not only in individual predictions but also in the adaptation process itself. This capability would enable practitioners to verify that model changes align with their domain knowledge about expected distributional shifts, identify when the classifier has detected a genuine concept change versus responded to noise, and maintain an auditable history of how the decision logic has evolved throughout the data stream.

This paper introduces \textbf{EGIS} (Evolutionary Grammar for Interpretable Streams), a novel approach that addresses both interpretability requirements in data stream classification. EGIS leverages grammatical evolution~\cite{oneill2003grammatical} to maintain a population of rule-based classifiers expressed as human-readable IF-THEN rules, enabling domain experts to understand model decisions without specialized machine learning knowledge. The evolutionary process adapts these rules as new data arrives, with a multi-level self-adaptation framework that automatically adjusts parameters based on population diversity, performance history, and drift severity. A specialized mutation mechanism called Gene Therapy accelerates drift recovery by injecting discriminative patterns extracted from auxiliary decision trees. Most importantly, EGIS introduces novel transition metrics that quantify how rules evolve between consecutive time periods, providing unprecedented insight into the adaptation process.

The research questions addressed in this paper are formulated as follows:

\begin{itemize}
    \item[\textbf{RQ1:}] Can a grammatical evolution approach achieve competitive predictive performance on data streams with concept drift while maintaining complete interpretability?

    \item[\textbf{RQ2:}] How can the adaptation behavior of a rule-based classifier be quantified and analyzed to understand not only what changes occur but where, how, and when these changes manifest?

    \item[\textbf{RQ3:}] Does a multi-level self-adaptation framework that responds to diversity, performance, and drift signals improve classifier robustness across different types of concept drift?
\end{itemize}

The main contributions of this paper are as follows:

\begin{enumerate}
    \item \textbf{Interpretable Rule Evolution Framework}: We propose a grammatical evolution approach that generates explicit IF-THEN classification rules, enabling domain experts to understand model decisions and verify classifier behavior without specialized machine learning knowledge.

    \item \textbf{Novel Transition Metrics}: We introduce three metrics, namely Transition Change Score (TCS), Rule Instability Rate (RIR), and Average Modification Severity (AMS), that quantify rule evolution dynamics, answering where, how, and when structural changes occur during concept drift adaptation.

    \item \textbf{Multi-Level Self-Adaptation Framework}: We develop a comprehensive adaptation system with fifteen mechanisms that respond to population diversity, performance history, and drift severity signals, enabling automatic parameter adjustment without manual tuning.

    \item \textbf{Gene Therapy Mutation}: We propose a knowledge-guided mutation operator that extracts discriminative patterns from specialized decision trees and injects them directly into the evolutionary population, accelerating convergence during drift recovery.

    \item \textbf{Comprehensive Empirical Evaluation}: We conduct extensive experiments spanning 48 datasets across five drift categories (abrupt, gradual, noisy, stationary, and real-world streams), evaluating EGIS under six configurations that systematically vary chunk size (500, 1000, 2000 instances) and complexity penalty settings. The resulting experimental matrix comprises over 300 individual runs, compared against eight state-of-the-art methods with rigorous statistical validation including Friedman tests and pairwise Wilcoxon signed-rank tests with Bonferroni correction.
\end{enumerate}

The remainder of this paper is organized as follows. Section~\ref{sec:problem} presents the formal problem formulation establishing the mathematical framework for data stream classification with interpretability requirements. Section~\ref{sec:related} reviews related work on grammatical evolution, rule-based stream classifiers, and concept drift adaptation. Section~\ref{sec:method} describes the proposed EGIS method in detail, including the grammar-based representation, multi-objective fitness function, evolutionary operators, self-adaptation framework, and transition metrics. Section~\ref{sec:experiments} presents the experimental setup, and Section~\ref{sec:results} discusses the results. Finally, Section~\ref{sec:conclusion} concludes the paper and outlines directions for future work.


%==============================================================================
\section{Problem Formulation}
\label{sec:problem}
%==============================================================================

We formalize the problem of interpretable data stream classification with concept drift adaptation, establishing the mathematical framework that underlies the proposed approach. Let $\mathcal{S} = \{(\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), \ldots\}$ denote an infinite data stream where each instance $(\mathbf{x}_t, y_t)$ arrives at discrete time step $t$, with $\mathbf{x}_t \in \mathcal{X} \subseteq \mathbb{R}^d$ representing a $d$-dimensional feature vector and $y_t \in \mathcal{Y} = \{c_1, c_2, \ldots, c_K\}$ denoting the class label from a finite set of $K$ classes. The stream is generated according to a time-varying joint probability distribution $P_t(\mathbf{x}, y)$ that may change at unknown time points, a phenomenon known as concept drift.

Concept drift occurs when the joint distribution changes over time, formally expressed as $\exists t_1, t_2: P_{t_1}(\mathbf{x}, y) \neq P_{t_2}(\mathbf{x}, y)$. This change may manifest through different mechanisms: real concept drift affects the posterior probability $P(y|\mathbf{x})$, altering the true decision boundary between classes, while virtual drift affects only the feature distribution $P(\mathbf{x})$ without changing the class-conditional relationships~\cite{gama2014survey}. The temporal dynamics of drift further distinguish abrupt changes, where the distribution shifts instantaneously at a single time point, from gradual drift, where the transition occurs smoothly over an extended period. Recurring concepts represent a special case where previously observed distributions reappear after an intervening period.

For computational tractability, the data stream is typically processed in discrete chunks $\mathcal{D}_i = \{(\mathbf{x}_t, y_t) : t \in [(i-1) \cdot n + 1, i \cdot n]\}$, where $n$ denotes the chunk size and $i$ indexes consecutive chunks. The chunk-based processing paradigm enables bounded memory usage while providing sufficient data for meaningful model updates. The classifier $f: \mathcal{X} \rightarrow \mathcal{Y}$ is trained on each chunk $\mathcal{D}_i$ and evaluated on the subsequent chunk $\mathcal{D}_{i+1}$, following a \textbf{train-then-test} protocol. Unlike the prequential (test-then-train) approach commonly used in online learning where each instance is first used for testing before training, the train-then-test protocol evaluates the classifier on completely unseen data from the next temporal window. This methodology provides a more realistic assessment of generalization to future data, as the test set represents genuinely held-out instances rather than individual samples interleaved with training.

The interpretability requirement constrains the hypothesis space to rule-based classifiers of the form $\mathcal{R} = \{r_1, r_2, \ldots, r_m\}$, where each rule $r_k$ takes the form:
\begin{equation}
    r_k: \text{IF } \phi_k(\mathbf{x}) \text{ THEN } c_k
\end{equation}
Here $\phi_k: \mathcal{X} \rightarrow \{0, 1\}$ is a Boolean function composed of conjunctions and disjunctions of atomic conditions, and $c_k \in \mathcal{Y}$ is the consequent class. Each atomic condition compares an attribute value against a threshold using a relational operator:
\begin{equation}
    \text{attr}_j \circ v, \quad \circ \in \{<, >, \leq, \geq, =, \neq\}
\end{equation}
This representation ensures that any evolved classifier can be directly inspected and understood by domain experts, as each rule expresses its decision logic in explicit terms that require no machine learning expertise to interpret.

The transition monitoring problem extends the standard classification task to encompass analysis of classifier evolution. Given consecutive rule sets $\mathcal{R}_{t-1}$ and $\mathcal{R}_t$ evolved for chunks $\mathcal{D}_{t-1}$ and $\mathcal{D}_t$ respectively, we seek to quantify: (1) the overall magnitude of structural change, (2) the proportion of rules that were added, modified, or deleted, and (3) the severity of modifications to individual rules. This quantification enables practitioners to understand not only the current state of the classifier but also how it arrived at that state through incremental adaptation.


%==============================================================================
\section{Related Work}
\label{sec:related}
%==============================================================================

The literature relevant to this work spans grammatical evolution for classification, rule-based methods for data streams, concept drift detection and adaptation, and explainability in machine learning. Grammatical evolution represents a form of genetic programming that uses a context-free grammar to constrain the search space to syntactically valid programs~\cite{oneill2003grammatical}. The grammar specifies production rules that map genotype representations to phenotype expressions, ensuring that all evolved solutions conform to the desired structural properties. This approach has found application in symbolic regression, automatic programming, and classification rule learning, where the grammar can encode domain-specific constraints on valid rule structures. The separation of genotype and phenotype enables standard genetic operators to work on integer chromosomes while the grammar mapping produces well-formed classification rules. Research has demonstrated that grammatical evolution can discover interpretable classifiers that rival neural networks on certain problems while maintaining the transparency that neural approaches lack~\cite{ryan1998grammatical}.

Rule-based classifiers for data streams have received increasing attention as the demand for interpretable machine learning grows. The Very Fast Decision Rules (VFDR) algorithm~\cite{gama2003accurate} extends Hoeffding bounds to rule learning, enabling incremental rule induction from streaming data with provable guarantees. G-eRules~\cite{shaker2012evolving} employs grammatical evolution to learn rules from streams, demonstrating that evolutionary approaches can achieve competitive performance while maintaining interpretability. ERulesD2S~\cite{de2019erulesd2s} extends this line of work by incorporating mechanisms for concept drift detection and rule adaptation, representing the closest prior work to our approach. However, ERulesD2S lacks the comprehensive self-adaptation framework and transition analysis capabilities that distinguish EGIS. The ACDWM algorithm~\cite{lu2017adaptive} employs a weighted majority ensemble of chunk-based classifiers, though its ensemble nature limits interpretability despite using rule learners as base classifiers.

Concept drift detection and adaptation mechanisms have evolved substantially since early work on the problem. The ADWIN algorithm~\cite{bifet2007learning} maintains a sliding window that automatically grows or shrinks based on detecting distribution changes, providing a principled approach to forgetting obsolete data. The Drift Detection Method (DDM)~\cite{gama2004learning} monitors the online error rate and signals drift when the error exceeds statistical thresholds. More recent approaches employ ensemble diversity~\cite{minku2011ddd} or explicit concept representations~\cite{goncalves2014comparative} to detect and respond to drift. Recent work has also explored the interplay between concept drift and class imbalance, with CDCMS.CIL~\cite{chiu2025cdcms} employing clustering in model space to maintain ensemble diversity under non-stationary class distributions. The severity-based approach in EGIS differs from binary drift detection by quantifying the magnitude of change and calibrating the adaptation response accordingly, enabling fine-grained control over the exploration-exploitation balance during recovery.

Ensemble methods dominate current benchmarks for data stream classification, achieving state-of-the-art accuracy through the aggregation of diverse base learners. Adaptive Random Forest (ARF)~\cite{gomes2017adaptive} combines online bagging with adaptive Hoeffding trees, using drift detectors to trigger tree replacement when performance degrades. Streaming Random Patches (SRP)~\cite{gomes2019streaming} employs random subspaces and online bagging to create diverse ensembles that are robust to various drift types. ROSE~\cite{cano2022rose} specifically addresses imbalanced data streams through adaptive oversampling techniques within an ensemble framework. While these methods achieve impressive predictive performance, their ensemble nature renders individual predictions unexplainable, a fundamental limitation that EGIS addresses through its rule-based representation.

The broader field of explainable artificial intelligence (XAI) has emphasized the importance of intrinsically interpretable models over post-hoc explanations~\cite{rudin2019stop}. Post-hoc methods such as LIME and SHAP provide local explanations for black-box predictions but do not guarantee that the explanation faithfully represents the model's actual reasoning process. In contrast, intrinsically interpretable models like rule sets and decision lists produce predictions through explicit logic that can be directly inspected. The data streaming context adds a temporal dimension to interpretability that has received limited attention: beyond understanding individual predictions, practitioners need to understand how the model's decision logic evolves over time. The transition metrics proposed in this paper address this gap by quantifying rule evolution dynamics throughout the stream.


%==============================================================================
\section{Proposed Method: EGIS}
\label{sec:method}
%==============================================================================

EGIS employs grammatical evolution to maintain a population of rule-based classifiers that adapt to concept drift through a sophisticated interplay of selection, crossover, mutation, and self-adaptation mechanisms. This section presents the method in detail, beginning with the grammar-based rule representation and proceeding through the fitness function, evolutionary operators, self-adaptation framework, and transition metrics. The complete processing pipeline is summarized in Algorithm~\ref{alg:egis}, followed by detailed textual descriptions of each phase.

\subsection{Grammar-Based Rule Representation}

The foundational representation scheme in EGIS employs a context-free grammar $G = (V_N, V_T, P, S)$ to generate classification rules that are inherently interpretable. The non-terminal symbols $V_N$ define the structural elements of rules, including logical operators and comparison constructs. The terminal symbols $V_T$ comprise the concrete elements that appear in final rules: attribute names drawn from the feature space, comparison operators appropriate to each attribute type, and threshold values within valid ranges. The production rules $P$ specify how non-terminals expand into combinations of terminals and non-terminals, while the start symbol $S$ initiates the derivation process.

The grammar enforces syntactic validity through carefully designed production rules that combine logical conditions with comparison operators. In Backus-Naur Form (BNF), the grammar is expressed as:
\begin{align}
    \langle \text{ruleset} \rangle &::= \langle \text{rule} \rangle \mid \langle \text{rule} \rangle \langle \text{ruleset} \rangle \nonumber \\
    \langle \text{rule} \rangle &::= \text{IF } \langle \text{cond} \rangle \text{ THEN } \langle \text{class} \rangle \nonumber \\
    \langle \text{cond} \rangle &::= \langle \text{term} \rangle \mid \langle \text{term} \rangle \langle \text{logop} \rangle \langle \text{cond} \rangle \nonumber \\
    \langle \text{term} \rangle &::= \langle \text{attr} \rangle \langle \text{compop} \rangle \langle \text{value} \rangle \nonumber \\
    \langle \text{logop} \rangle &::= \text{AND} \mid \text{OR} \nonumber \\
    \langle \text{compop} \rangle &::= < \mid > \mid \leq \mid \geq \mid = \mid \neq
\end{align}

Each individual in the population represents a complete rule set $\mathcal{R} = \{r_1, r_2, \ldots, r_m\}$ capable of classifying instances across all classes. The representation follows a tree-based structure where internal nodes contain non-terminal symbols (logical operators, comparisons) while leaf nodes contain terminals (attributes, values, operators). This structure enables natural genetic operations while maintaining syntactic validity; any subtree exchange or node mutation produces a rule that conforms to the grammar specification. The complexity of each rule is measured through tree depth $d(r)$ and condition count $|\phi(r)|$, providing explicit control over the expressiveness-interpretability trade-off.

The internal representation employs Abstract Syntax Trees (ASTs) to capture the hierarchical structure of rule conditions. Each rule is parsed into an AST where internal nodes represent logical operators (AND, OR) and leaf nodes contain atomic conditions of the form $\text{attr}_j \circ v$. This tree-based representation enables efficient rule manipulation during evolutionary operations and provides a canonical form for rule comparison. For commutative operators, operands are lexicographically sorted to ensure that logically equivalent rules receive identical representations; for instance, $(\text{attr}_1 > 5) \land (\text{attr}_2 < 3)$ and $(\text{attr}_2 < 3) \land (\text{attr}_1 > 5)$ yield the same canonical AST. This normalization facilitates accurate similarity computation in the transition metrics, as semantically equivalent rules are recognized as identical rather than spuriously different.

The grammar adapts to the feature space by instantiating attribute-specific productions. Numeric attributes use comparison operators $\{<, >, \leq, \geq\}$ with threshold values sampled from the observed range, while categorical attributes use equality operators $\{=, \neq\}$ with values from the attribute domain. This type-aware generation ensures that all evolved conditions are semantically meaningful for the given dataset. A bias toward conjunctive rules (80\% probability for AND versus 20\% for OR at each expansion) produces more specific rules that are typically easier for humans to interpret.


\subsection{Multi-Objective Fitness Function}

The fitness function balances three fundamental objectives that govern rule quality, with dynamic weighting that adapts based on drift context. The primary objective captures predictive accuracy through the geometric mean of class-specific recalls, a metric that ensures balanced performance across classes:
\begin{equation}
    \text{G-Mean} = \left( \prod_{k=1}^{K} \text{Recall}_k \right)^{1/K}
    \label{eq:gmean}
\end{equation}
where $\text{Recall}_k = \text{TP}_k / (\text{TP}_k + \text{FN}_k)$ for class $k$. The geometric mean penalizes classifiers that achieve high accuracy on majority classes while neglecting minority classes, an important consideration for imbalanced data streams.

The coverage objective incentivizes rules that classify a larger proportion of instances, preventing overly specific rules that match few examples:
\begin{equation}
    \text{Coverage}(\mathcal{R}) = \frac{|\{(\mathbf{x}, y) \in \mathcal{D} : \exists r \in \mathcal{R}, \phi_r(\mathbf{x}) = 1\}|}{|\mathcal{D}|}
\end{equation}
A class-weighted coverage bonus further rewards rules that cover underrepresented classes, with weights inversely proportional to class frequencies.

The complexity penalization objective favors simpler, more interpretable structures:
\begin{equation}
    \text{Complexity}(\mathcal{R}) = \lambda_r \cdot |\mathcal{R}| + \lambda_c \cdot \sum_{r \in \mathcal{R}} |\phi(r)| + \lambda_f \cdot |A(\mathcal{R})|
\end{equation}
where $|\mathcal{R}|$ counts total rules, $|\phi(r)|$ counts conditions in rule $r$, and $|A(\mathcal{R})|$ counts distinct attributes used across all rules. The coefficients $\lambda_r$, $\lambda_c$, and $\lambda_f$ control the relative importance of each complexity component.

The complete fitness function combines these objectives with stability penalties that discourage unnecessary structural changes:
\begin{equation}
    F(\mathcal{R}) = \alpha \cdot \text{G-Mean} + \beta_c \cdot \text{Cov} - \gamma \cdot \text{Cplx} - \beta_s \cdot \text{Stab}
    \label{eq:fitness}
\end{equation}
where Cov, Cplx, and Stab denote Coverage, Complexity, and Stability respectively.
The stability penalty measures feature distance from a reference configuration using the Jaccard distance:
\begin{equation}
    \text{Stability}(\mathcal{R}) = d_J(A(\mathcal{R}), A_{\text{ref}}) = \frac{|A(\mathcal{R}) \triangle A_{\text{ref}}|}{|A(\mathcal{R}) \cup A_{\text{ref}}|}
\end{equation}
where $A_{\text{ref}}$ denotes the features used by the best individual from the previous generation.

The coefficients governing these objectives are not static but adapt dynamically based on drift severity. When significant concept change is detected, stability constraints are relaxed to enable exploration of new rule structures. This adaptive weighting represents a key mechanism for balancing exploitation of learned knowledge with exploration of emerging patterns. Specifically, when drift severity exceeds a threshold, the stability penalty coefficient $\beta_s$ is reduced to zero, and the complexity penalty is suspended, allowing the evolutionary process to discover radically different rule structures if needed.

An early termination mechanism conserves computational resources by interrupting evaluation of clearly inferior individuals. The first 20\% of training instances (minimum 100) are evaluated, and if the partial G-Mean falls below 50\% of the median fitness among elite individuals, evaluation terminates immediately. This heuristic reduces wasted computation on non-viable candidates, enabling more thorough evaluation of promising individuals.


\subsection{Evolutionary Operators}

The evolutionary process relies on carefully designed operators that balance exploration of the search space with exploitation of discovered solutions. Selection operates through a tournament mechanism with dynamically adjusted pressure. When population diversity is high, indicating risk of premature convergence, stronger selection pressure intensifies competition among individuals. When diversity diminishes, reduced pressure preserves alternative solutions that may prove valuable if conditions change. The tournament size varies according to:
\begin{equation}
    k_t = k_{\min} + \delta \cdot (k_{\max} - k_{\min})
\end{equation}
where $\delta$ measures population diversity as the normalized fitness dispersion, and $k_{\min}=2$, $k_{\max}=5$ define the range.

The crossover operator implements a two-mode strategy that adapts throughout evolution. During early generations when class coverage is incomplete, \textit{expansion mode} performs subtree exchanges that promote broad exploration of the search space. Two parent individuals are selected, random rules are chosen from each, and subtrees are swapped at compatible crossover points. This aggressive exploration helps discover rules covering classes that neither parent adequately addresses. As evolution progresses and populations achieve complete class coverage, the system transitions to \textit{refinement mode}, which transfers high-quality rule components between individuals to intensify search in promising regions. For each class, the worst-performing rule in the child is identified along with the best-performing rule from the second parent; if the parent's rule exceeds the child's rule in quality, replacement occurs. This quality-directed transfer mechanism preserves good components while improving weak spots.

Mutation operates at three structural levels within rules. Operator mutation alters comparison operators with probability $p_{\text{op}}$, enabling fine-grained adjustments to decision boundaries, for example, changing ``$\text{attr}_j > v$'' to ``$\text{attr}_j \geq v$''. Value mutation perturbs thresholds within valid ranges, allowing smooth adaptation to shifting data distributions. A mutation of magnitude $\epsilon$ drawn from a Gaussian distribution centered at zero adjusts the threshold while respecting attribute bounds. Subtree mutation replaces entire rule components with newly generated structures, introducing substantial novelty when incremental adjustments prove insufficient. The overall mutation rate adapts inversely to population diversity:
\begin{equation}
    p_m = p_m^{\text{base}} + (1 - \delta) \cdot (p_m^{\max} - p_m^{\text{base}})
\end{equation}
ensuring that mutation intensity increases as the population converges.

A specialized mutation mechanism, termed \textbf{Gene Therapy}, leverages knowledge extracted directly from training data to accelerate convergence. This approach trains a specialized decision tree for each class, creating a focused binary classifier that distinguishes the target class from all others. The tree is constrained to shallow depth (maximum 5 levels) to produce interpretable rules. Paths from root to leaf in this auxiliary tree are extracted as candidate rules, each representing a conjunction of conditions that discriminate the target class. These candidate rules are scored by a function combining classification confidence with instance coverage:
\begin{equation}
    \text{Score}(r) = \text{Purity}(r) \cdot \log(1 + \text{Support}(r))
\end{equation}
where $\text{Purity}(r)$ measures the proportion of correctly classified instances among those matching the rule, and $\text{Support}(r)$ counts the matching instances. The highest-scoring rule replaces the worst-performing rule for the corresponding class in the target individual. This mechanism injects discriminative knowledge directly into the evolutionary population, providing a shortcut to good solutions that pure random mutation might take many generations to discover. Gene Therapy is particularly valuable during drift recovery, when new patterns must be rapidly incorporated into the classifier.

An additional intensification mechanism, \textbf{Hill Climbing}, applies local search to elite individuals. Three strategies operate based on performance classification: aggressive intensification for poor performers (G-Mean below 85\%) generates many variants through error-focused decision tree extraction; moderate intensification (85-96\%) combines boosted ensemble predictions; fine-tuning (above 96\%) applies guided mutations weighted by feature importance. This hierarchical approach allocates computational effort according to the improvement potential of each individual.


\subsection{Self-Adaptation Framework}

EGIS implements a multi-level self-adaptation system that automatically adjusts parameters and strategies in response to three complementary signals: population diversity, performance history, and drift severity. This framework eliminates the need for manual parameter tuning across different datasets and drift scenarios, enabling robust performance without domain-specific configuration.

Diversity-based adaptation monitors population convergence through fitness value dispersion. The diversity metric $\delta$ is computed as the normalized range of fitness values among population members. When diversity diminishes below a threshold, indicating premature convergence risk, mutation rate increases automatically to reintroduce variability. Simultaneously, selection pressure decreases to preserve alternative solutions that might otherwise be eliminated by strong competition. The crossover mode transition from expansion to refinement also responds to diversity signals: high diversity suggests the population has not yet converged on a promising region, warranting continued exploration.

Performance-based adaptation classifies predictive performance into three levels based on recent history and absolute thresholds. Good performance (G-Mean exceeds the historical mean plus one standard deviation, or exceeds an absolute threshold of 0.85) indicates the current rule set effectively captures the underlying concept, warranting conservative exploration with substantial inheritance from previous populations. Medium performance (within one standard deviation of the mean) suggests the classifier is adequate but could benefit from moderate exploration. Poor performance (below the mean minus one standard deviation, or below an absolute threshold of 0.30) signals potential concept obsolescence, triggering increased injection of new individuals and reduced reliance on inherited knowledge. The stability penalty coefficient $\beta_s$ adapts according to performance classification: high values (0.05) for good performance encourage stability, while low values (0.01) for poor performance permit structural change.

Drift-based adaptation employs a severity analysis framework that classifies detected changes into four levels. The drift severity metric combines three complementary signals:
\begin{equation}
    S_{\text{drift}} = 1 - \left( 0.5 \cdot \text{sim}_\mu + 0.3 \cdot \text{sim}_\pi + 0.2 \cdot \text{sim}_\sigma \right)
\end{equation}
where $\text{sim}_\mu$ measures cosine similarity between feature means across chunks, $\text{sim}_\pi$ measures $L_1$ similarity between class distributions, and $\text{sim}_\sigma$ measures cosine similarity between feature standard deviations. This multi-faceted metric captures different aspects of distribution change: feature location shift, class proportion change, and feature dispersion change.

Based on the computed severity, the system classifies drift into four levels and triggers corresponding responses:
\begin{itemize}
    \item \textbf{Stable} ($S_{\text{drift}} < 0.05$): Normal operation with full penalty enforcement and standard parameters.
    \item \textbf{Mild} ($0.05 \leq S_{\text{drift}} < 0.10$): Subtle parameter adjustments without disrupting the learned model.
    \item \textbf{Moderate} ($0.10 \leq S_{\text{drift}} < 0.25$): Stability penalties removed, evolutionary budget potentially increased to 15 additional generations.
    \item \textbf{Severe} ($S_{\text{drift}} \geq 0.25$): Partial knowledge reset with mutation rate elevated to 0.5, 60\% of population reinitialized randomly, and up to 25 additional recovery generations.
\end{itemize}

The adaptive memory management system maintains two complementary structures. A best-solutions memory stores high-fitness individuals encountered throughout the stream, pruned periodically to retain the most recent and highest-quality solutions. When critical performance degradation indicates stored knowledge has become obsolete (performance drop exceeding 55\% over consecutive chunks), this memory is automatically cleared through an abandonment mechanism. A concept fingerprint memory stores statistical summaries of processed concepts, enabling detection of recurring patterns. Each fingerprint captures the mean vector, standard deviation vector, and class distribution of a chunk. When a new chunk arrives, its fingerprint is compared against stored fingerprints; if similarity exceeds 0.85, a recurring concept is identified, and previously successful solutions are restored from concept memory. This mechanism dramatically accelerates re-adaptation when concepts recur, a common phenomenon in many real-world streams.

Population initialization follows an adaptive recipe determined by problem complexity and drift context. A rapid probe using a shallow decision tree (maximum depth 3) estimates current problem complexity by measuring the probe's accuracy on the current chunk. High probe accuracy (above 0.90) indicates a simple problem where decision tree seeding should dominate (80\% of population). Medium probe accuracy (0.75-0.90) suggests moderate complexity with 60\% seeding. Low probe accuracy (below 0.75) indicates a complex problem where random initialization should predominate (60\%) to ensure adequate exploration. The final recipe balances inherited knowledge (from memory and previous population elite) with exploration (seeded and random individuals) according to both complexity assessment and current drift context.

A distinguishing characteristic of EGIS is its reliance on \textbf{implicit drift detection} rather than explicit statistical tests. Unlike methods that employ dedicated drift detectors such as ADWIN~\cite{bifet2007learning} or DDM~\cite{gama2004learning} to trigger model updates, EGIS continuously adapts through the interplay of fingerprint comparison and performance monitoring. The severity framework (Equation~\ref{eq:fitness}) quantifies distributional change without requiring binary drift/no-drift decisions, enabling graduated responses proportional to the magnitude of change. This design philosophy avoids the sensitivity-specificity trade-off inherent in threshold-based detection: excessively sensitive detectors trigger frequent false alarms and unnecessary adaptation, while conservative thresholds may delay response to genuine drift. By treating drift detection and adaptation as a unified continuous process, EGIS achieves robust performance across drift scenarios without detector tuning.


\subsection{Transition Metrics}

The transition metrics constitute a central contribution of this work, quantifying rule evolution dynamics between consecutive chunks and answering three fundamental questions about classifier adaptation. Unlike standard accuracy metrics that reveal only predictive performance at a single time point, these metrics characterize the adaptation process itself, enabling practitioners to understand how and why the classifier changes over time.

The \textbf{Rule Instability Rate (RIR)} measures the proportion of rules that were structurally modified between consecutive time points. To compute RIR, rules from consecutive chunks $\mathcal{R}_{t-1}$ and $\mathcal{R}_t$ are matched using a similarity function based on normalized Levenshtein distance between rule string representations. Rules exceeding a similarity threshold $\tau$ (typically 0.35) are considered modifications of the same underlying rule; below this threshold, they are treated as complete replacements. RIR then counts the proportion of rules that were either added (present in $\mathcal{R}_t$ but with no similar match in $\mathcal{R}_{t-1}$) or deleted (present in $\mathcal{R}_{t-1}$ but with no similar match in $\mathcal{R}_t$):
\begin{equation}
    \text{RIR} = \frac{|\text{Added}| + |\text{Deleted}|}{|\mathcal{R}_{t-1}| + |\mathcal{R}_t|}
    \label{eq:rir}
\end{equation}
This metric indicates \textit{where} structural changes occurred in the rule set, identifying which classifier components were replaced during adaptation. High RIR values indicate substantial structural turnover, while low values indicate stability in the rule set composition.

The \textbf{Average Modification Severity (AMS)} quantifies the degree of alteration for rules that were modified rather than completely replaced. For each matched rule pair $(r_{t-1}, r_t)$ where similarity exceeds $\tau$, the modification severity is computed as one minus the similarity:
\begin{equation}
    \text{AMS} = \frac{1}{|\text{Modified}|} \sum_{(r_{t-1}, r_t) \in \text{Modified}} \left(1 - \text{sim}(r_{t-1}, r_t)\right)
    \label{eq:ams}
\end{equation}
The similarity function employs normalized Levenshtein edit distance:
\begin{equation}
    \text{sim}(r_1, r_2) = 1 - \frac{\text{edit\_distance}(\text{str}(r_1), \text{str}(r_2))}{\max(|\text{str}(r_1)|, |\text{str}(r_2)|)}
\end{equation}
AMS indicates \textit{how} individual rules changed, capturing the magnitude of modifications realized during adaptation. High AMS with low RIR suggests gradual refinement of existing rules; low AMS with high RIR suggests complete rule replacement.

The \textbf{Transition Change Score (TCS)} combines instability and severity into a unified metric that characterizes overall adaptation intensity:
\begin{equation}
    \text{TCS} = w_1 \cdot \text{RIR} + w_2 \cdot (1 - \text{RIR}) \cdot \text{AMS}
    \label{eq:tcs}
\end{equation}
where default weights $w_1 = 0.6$ and $w_2 = 0.4$ balance the contributions of complete replacements (captured by RIR) and partial modifications (captured by the product of retention proportion and AMS). TCS indicates \textit{when} significant transitions occurred, enabling identification of inflection points in classifier adaptation. A TCS near 1.0 indicates radical transformation of the rule set; TCS near 0.0 indicates minimal change.

Joint interpretation of these metrics enables rich analysis of adaptation behavior. Peaks in TCS signal moments of significant change that may correspond to concept drift events. High RIR with low AMS indicates a rule replacement strategy where entire rules are swapped rather than modified. Low RIR with high AMS indicates a gradual refinement strategy where existing rules are incrementally adjusted. Sequences of low TCS indicate stable concepts where the classifier requires minimal adaptation. When TCS spikes following a period of stability, practitioners can examine which rules changed and how, correlating these changes with domain knowledge about expected distributional shifts.


\subsection{Algorithm Description}

Algorithm~\ref{alg:egis} presents the complete EGIS processing pipeline. The algorithm processes the data stream in discrete chunks, updating the classifier as each chunk arrives.

\begin{algorithm}[H]
\small
\caption{EGIS: Evolutionary Grammar for Interpretable Streams}
\label{alg:egis}
\begin{algorithmic}[1]
\REQUIRE Data stream $\mathcal{S}$, chunk size $n$, population size $N$
\ENSURE Sequence of classifiers $\{f_1, f_2, \ldots\}$ and transition metrics

\STATE Initialize grammar $G$ from feature space
\STATE Initialize empty memories: $\mathcal{M}_{\text{best}} \leftarrow \emptyset$, $\mathcal{M}_{\text{concept}} \leftarrow \emptyset$
\STATE Initialize previous rules $\mathcal{R}_{\text{prev}} \leftarrow \emptyset$

\FOR{each chunk $\mathcal{D}_i$ from stream $\mathcal{S}$}
    \STATE \textbf{// Phase 1: Fingerprint and Recurrence Detection}
    \STATE $\text{fp}_i \leftarrow \text{ComputeFingerprint}(\mathcal{D}_i)$
    \STATE $\text{recurring} \leftarrow \text{DetectRecurrence}(\text{fp}_i, \mathcal{M}_{\text{concept}})$

    \STATE \textbf{// Phase 2: Drift Severity Classification}
    \STATE $S_{\text{drift}} \leftarrow \text{ComputeDriftSeverity}(\text{fp}_i, \text{fp}_{i-1})$
    \STATE $\text{level} \leftarrow \text{ClassifyDrift}(S_{\text{drift}})$
    \STATE Adjust penalties and parameters based on $\text{level}$

    \STATE \textbf{// Phase 3: Population Initialization}
    \STATE $\text{complexity} \leftarrow \text{ProbeComplexity}(\mathcal{D}_i)$
    \STATE $\mathcal{P}_0 \leftarrow \text{InitializePopulation}(N, \text{complexity}, \text{recurring}, \mathcal{M}_{\text{best}})$

    \STATE \textbf{// Phase 4: Evolutionary Optimization}
    \FOR{generation $g = 1$ to $g_{\max}$}
        \STATE Evaluate fitness: $F(p) \leftarrow \text{Equation~\ref{eq:fitness}}$ for all $p \in \mathcal{P}_{g-1}$
        \STATE $\delta \leftarrow \text{ComputeDiversity}(\mathcal{P}_{g-1})$
        \STATE Adjust tournament size and mutation rate based on $\delta$

        \STATE $\mathcal{P}_{\text{elite}} \leftarrow \text{SelectElite}(\mathcal{P}_{g-1}, 0.1 \cdot N)$
        \STATE $\mathcal{P}_{\text{parents}} \leftarrow \text{TournamentSelection}(\mathcal{P}_{g-1})$
        \STATE $\mathcal{P}_{\text{offspring}} \leftarrow \text{AdaptiveCrossover}(\mathcal{P}_{\text{parents}}, \delta)$
        \STATE $\mathcal{P}_{\text{mutated}} \leftarrow \text{Mutation}(\mathcal{P}_{\text{offspring}}, p_m)$
        \STATE $\mathcal{P}_{\text{mutated}} \leftarrow \text{GeneTherapy}(\mathcal{P}_{\text{mutated}}, \mathcal{D}_i)$
        \STATE $\mathcal{P}_g \leftarrow \mathcal{P}_{\text{elite}} \cup \mathcal{P}_{\text{mutated}}$

        \IF{stagnation detected}
            \STATE Apply Hill Climbing to elite individuals
        \ENDIF
    \ENDFOR

    \STATE \textbf{// Phase 5: Recovery (if severe drift)}
    \IF{$\text{level} = \text{SEVERE}$}
        \STATE Execute additional recovery generations with elevated mutation
    \ENDIF

    \STATE \textbf{// Phase 6: Output and Memory Update}
    \STATE $f_i \leftarrow \text{BestIndividual}(\mathcal{P}_{g_{\max}})$
    \STATE $\mathcal{R}_i \leftarrow \text{ExtractRules}(f_i)$

    \STATE \textbf{// Phase 7: Transition Metrics}
    \STATE Compute RIR, AMS, TCS using Equations~\ref{eq:rir}-\ref{eq:tcs}
    \STATE $\mathcal{R}_{\text{prev}} \leftarrow \mathcal{R}_i$

    \STATE \textbf{// Phase 8: Memory Management}
    \STATE Update $\mathcal{M}_{\text{best}}$ with elite individuals
    \STATE Store $(\text{fp}_i, \text{elite})$ in $\mathcal{M}_{\text{concept}}$
    \IF{abandonment criteria met}
        \STATE $\mathcal{M}_{\text{best}} \leftarrow \emptyset$
    \ENDIF

    \STATE \textbf{Output:} Classifier $f_i$, Rules $\mathcal{R}_i$, Metrics (RIR, AMS, TCS)
\ENDFOR
\end{algorithmic}
\end{algorithm}

The algorithm begins by initializing the grammar from the feature space specification, creating production rules appropriate for each attribute type. Empty memory structures are created for storing high-fitness individuals and concept fingerprints. Processing then proceeds through eight phases for each chunk.

Phase 1 computes a statistical fingerprint of the current chunk and checks for recurring concepts by comparing against stored fingerprints. If a recurring concept is detected (similarity exceeding 0.85), previously successful solutions are retrieved from concept memory to accelerate re-adaptation.

Phase 2 computes drift severity by comparing the current fingerprint against the previous chunk's fingerprint. The severity score determines the adaptation level (stable, mild, moderate, or severe), which in turn controls penalty coefficients and evolutionary parameters. Severe drift triggers penalty suspension and parameter adjustments that favor exploration over stability.

Phase 3 initializes the population according to an adaptive recipe. A shallow decision tree probe estimates problem complexity, informing the proportion of seeded versus random individuals. Recurring concept detection influences whether to restore previously successful solutions. The final population combines inherited knowledge (from memory and previous elite) with exploration (seeded rules and random individuals) in proportions determined by the current context.

Phase 4 executes the main evolutionary loop. Each generation evaluates fitness using Equation~\ref{eq:fitness}, computes population diversity, and adjusts tournament size and mutation rate accordingly. Elite individuals (top 10\%) are preserved unchanged. Parents are selected through adaptive tournament selection, recombined through adaptive crossover (expansion or refinement mode based on class coverage), and mutated at the adjusted rate. Gene Therapy injects knowledge from specialized decision trees. If stagnation is detected (no fitness improvement for multiple generations), Hill Climbing applies intensive local search to elite individuals.

Phase 5 handles severe drift recovery. When drift classification indicates severe change, additional generations with elevated mutation rate (0.5) and increased random individual proportion (60\%) enable rapid exploration of the search space. This recovery mode continues until performance stabilizes or a maximum generation budget is exhausted.

Phases 6-8 finalize chunk processing. The best individual becomes the current classifier, and its rules are extracted for output and metric computation. Transition metrics (RIR, AMS, TCS) are computed by comparing current rules against the previous chunk's rules using Equations~\ref{eq:rir}-\ref{eq:tcs}. Memory structures are updated with elite individuals and the current fingerprint. If abandonment criteria are met (severe performance degradation over consecutive chunks), the best-solutions memory is cleared to prevent obsolete knowledge from contaminating future evolution.


\subsection{Explainability Analysis Tools}

EGIS provides a suite of analysis tools for examining adaptation dynamics beyond the transition metrics. The rule difference analyzer compares consecutive rule sets, identifying unchanged, modified, added, and deleted rules through Levenshtein similarity with configurable thresholds. It generates detailed textual reports listing each rule change with its similarity score, enabling practitioners to trace exactly how the classifier evolved between chunks. Evolution matrices visualize the modification patterns as heatmaps, with rows representing previous rules, columns representing current rules, and cell values indicating pairwise similarities.

The performance-based drift detector monitors accuracy throughout the stream, identifying significant drops that correlate with concept changes. When accuracy falls below the historical mean by more than one standard deviation, a drift warning is triggered. This detector operates independently of the statistical fingerprinting mechanism, providing complementary evidence for concept change.

The concept difference analyzer quantifies differences between concept definitions through label disagreement rates. Given two classifiers trained on different chunks, the analyzer measures how often they disagree on predictions for a held-out sample. This disagreement rate indicates the magnitude of concept change and informs the severity analysis framework. Disagreement matrices produced by this analyzer reveal which class pairs experience the most decision boundary shift, helping practitioners understand not just that drift occurred but which concepts were most affected.


%==============================================================================
\section{Experimental Setup}
\label{sec:experiments}
%==============================================================================

We evaluate EGIS through comprehensive experiments designed to assess both predictive performance and adaptation dynamics across diverse concept drift scenarios. The experimental corpus encompasses 48 datasets spanning abrupt, gradual, recurring, and noisy drift patterns, enabling systematic analysis of how the self-adaptation framework responds to different drift characteristics. This section describes the datasets employed, the comparative methods against which EGIS is benchmarked, the experimental configurations that vary chunk size and complexity penalty, and the evaluation metrics used to quantify performance and classifier stability.

\subsection{Datasets}

The experimental corpus comprises 48 data streams spanning five distinct categories designed to stress-test different aspects of adaptive classification. Synthetic datasets generated through the MOA framework enable precise evaluation under controlled drift conditions, while real-world datasets validate performance on authentic streaming scenarios with unknown distributional dynamics. Table~\ref{tab:datasets} organizes these datasets by drift category, distinguishing abrupt transitions (instantaneous distribution changes), gradual drift (extended transition periods), noisy drift (label corruption during transitions), stationary streams (baseline without drift), and real-world scenarios with undocumented drift characteristics.

\begin{table}[htbp]
\centering
\caption{Dataset Categories and Characteristics}
\label{tab:datasets}
\footnotesize
\begin{tabular}{p{2.2cm}p{5.3cm}}
\toprule
\textbf{Category} & \textbf{Datasets} \\
\midrule
\textbf{Abrupt Drift} &
SEA\_Abrupt\_Simple, SEA\_Abrupt\_Chain, SEA\_Abrupt\_Recurring, AGRAWAL\_Abrupt\_Simple\_Mild, AGRAWAL\_Abrupt\_Simple\_Severe, AGRAWAL\_Abrupt\_Chain\_Long, RBF\_Abrupt\_Blip, RBF\_Abrupt\_Severe, HYPERPLANE\_Abrupt\_Simple, STAGGER\_Abrupt\_Chain, STAGGER\_Abrupt\_Recurring, RANDOMTREE\_Abrupt\_Simple, RANDOMTREE\_Abrupt\_Recurring, SINE\_Abrupt\_Simple, LED\_Abrupt\_Simple, WAVEFORM\_Abrupt\_Simple \\
\midrule
\textbf{Gradual Drift} &
SEA\_Gradual\_Simple\_Fast, SEA\_Gradual\_Simple\_Slow, SEA\_Gradual\_Recurring, RBF\_Gradual\_Moderate, RBF\_Gradual\_Severe, HYPERPLANE\_Gradual\_Simple, STAGGER\_Gradual\_Chain, RANDOMTREE\_Gradual\_Simple, SINE\_Gradual\_Recurring, LED\_Gradual\_Simple, WAVEFORM\_Gradual\_Simple \\
\midrule
\textbf{Noisy Drift} &
AGRAWAL\_Abrupt\_Simple\_Severe\_Noise, RBF\_Abrupt\_Blip\_Noise, RBF\_Gradual\_Severe\_Noise, HYPERPLANE\_Gradual\_Noise, RANDOMTREE\_Gradual\_Noise, SEA\_Abrupt\_Chain\_Noise, STAGGER\_Abrupt\_Chain\_Noise, SINE\_Abrupt\_Recurring\_Noise \\
\midrule
\textbf{Stationary} &
AGRAWAL\_Stationary, SEA\_Stationary, RBF\_Stationary, HYPERPLANE\_Stationary, STAGGER\_Stationary, RANDOMTREE\_Stationary, SINE\_Stationary, LED\_Stationary, WAVEFORM\_Stationary \\
\midrule
\textbf{Real-World} &
Electricity, AssetNegotiation\_F2, AssetNegotiation\_F3, AssetNegotiation\_F4 \\
\bottomrule
\end{tabular}
\end{table}

All synthetic datasets were generated once using the MOA framework~\cite{bifet2010moa} with fixed random seeds to ensure reproducibility. Each dataset comprises 12,000 instances, subsequently partitioned into non-overlapping temporal chunks of 500, 1,000, or 2,000 instances depending on the experimental configuration. This pre-generation strategy guarantees that all comparative methods process identical data sequences, eliminating any confounding effects from stochastic data generation during evaluation. Concept drifts in synthetic streams occur at predetermined chunk boundaries, enabling precise analysis of adaptation behavior at known transition points.

Table~\ref{tab:dataset_sizes} summarizes the chunk structure and resulting evaluation counts for each configuration.

\begin{table}[htbp]
\centering
\caption{Dataset Dimensions and Chunk Structure}
\label{tab:dataset_sizes}
\footnotesize
\begin{tabular}{lcccc}
\toprule
\textbf{Chunk Size} & \textbf{Instances} & \textbf{Chunks} & \textbf{Evals} & \textbf{Runs} \\
\midrule
500 & 12,000 & 24 & 23 & 48 $\times$ 2 = 96 \\
1,000 & 12,000 & 12 & 11 & 48 $\times$ 2 = 96 \\
2,000 & 12,000 & 6 & 5 & 48 $\times$ 2 = 96 \\
\midrule
\multicolumn{4}{l}{\textbf{Total experimental runs:}} & \textbf{288} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Abrupt drift} streams feature instantaneous distribution changes at predetermined chunk boundaries, simulating scenarios such as sudden policy changes or system failures. The severity varies from mild (function parameter changes in AGRAWAL) to severe (complete concept replacement in RBF). \textbf{Gradual drift} streams transition between concepts over extended periods spanning multiple chunks, with transition speeds ranging from fast (100 instances) to slow (500 instances). \textbf{Recurring drift} streams cycle through previously observed concepts, testing the classifier's ability to recognize and rapidly re-adapt to familiar distributions. \textbf{Noisy drift} streams add 10-20\% label noise during transitions, stress-testing robustness against imperfect drift signals. \textbf{Stationary} streams provide baseline performance without distribution change, verifying that adaptive mechanisms do not degrade performance when adaptation is unnecessary. Real-world datasets represent genuine streaming scenarios from domains including electricity demand and asset negotiation.

\subsection{Comparative Methods}

We compare EGIS against eight state-of-the-art methods spanning ensemble approaches, tree-based methods, and interpretable classifiers:

\begin{itemize}
    \item \textbf{ROSE}~\cite{cano2022rose}: Robust Online Self-adjusting Ensemble for imbalanced streams, evaluated in both original and chunk-based evaluation configurations.
    \item \textbf{ARF}~\cite{gomes2017adaptive}: Adaptive Random Forest combining online bagging with adaptive Hoeffding trees.
    \item \textbf{SRP}~\cite{gomes2019streaming}: Streaming Random Patches employing random subspaces.
    \item \textbf{CDCMS.CIL}~\cite{chiu2025cdcms}: Concept Drift handling based on Clustering in Model Space for Class-Imbalanced Learning, a heterogeneous ensemble that maintains diversity through model-space clustering with G-Mean weighted voting.
    \item \textbf{HAT}~\cite{bifet2009adaptive}: Hoeffding Adaptive Tree with ADWIN change detection.
    \item \textbf{ACDWM}~\cite{lu2017adaptive}: Adaptive Chunk-based Dynamic Weighted Majority.
    \item \textbf{ERulesD2S}~\cite{de2019erulesd2s}: Evolutionary Rules for Data Streams, the only other interpretable baseline.
\end{itemize}

\textbf{Model Limitations and Scope.} Several comparative methods exhibit inherent limitations that affect their applicability across the full dataset corpus. ACDWM is designed exclusively for binary classification problems; when applied to multi-class datasets (LED and WAVEFORM generators), the algorithm encounters numerical instability during training in its underbagging component, which assumes strictly binary class labels. Following established practice in comparative studies~\cite{demsar2006statistical}, we assign G-Mean=0.0 to these failed cases, reflecting the method's inability to handle multi-class problems while maintaining transparency in ranking comparisons.

HAT exhibits systematic underfitting on smaller chunk sizes, with training G-Mean approximately 8\% lower than EGIS. This outcome is consistent with documented limitations of Hoeffding tree estimation, which requires substantial instance counts to reliably compute split statistics. ERulesD2S, while designed for interpretability, frequently produced collapsed classifiers assigning uniform predictions to all instances, generating no valid rules in our experimental configuration.

Among ensemble methods, ARF, SRP, ROSE, and CDCMS.CIL demonstrated robust performance across most dataset types but provide no interpretability. Their predictions emerge from aggregating multiple base learners through mechanisms that preclude explanation of individual decisions, representing the fundamental accuracy-interpretability trade-off that motivates EGIS development. CDCMS.CIL stands out among ensembles for its native G-Mean optimization and full multiclass support, making it a particularly strong baseline for class-imbalanced scenarios.

Table~\ref{tab:model_capabilities} summarizes the capabilities and limitations of each method.

\begin{table}[htbp]
\centering
\caption{Comparative Methods: Capabilities and Limitations}
\label{tab:model_capabilities}
\footnotesize
\begin{tabular}{lcccl}
\toprule
\textbf{Method} & \textbf{Multi.} & \textbf{Imbal.} & \textbf{Interp.} & \textbf{Type} \\
\midrule
EGIS & \checkmark & \checkmark & Full & Rules \\
ARF & \checkmark & Partial & None & Ensemble \\
SRP & \checkmark & Partial & None & Ensemble \\
CDCMS.CIL & \checkmark & Focus & None & Ensemble \\
HAT & \checkmark & Limited & Low & Tree \\
ROSE & \checkmark & \checkmark & None & Ensemble \\
ACDWM & -- & \checkmark & None & Ensemble \\
ERulesD2S & \checkmark & Limited & Medium & Rules \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Experimental Configurations}

The experimental design systematically varies chunk size and complexity penalty to evaluate adaptation behavior across different temporal granularities and interpretability constraints. Table~\ref{tab:exp_config} presents the six experimental configurations.

\begin{table}[htbp]
\centering
\caption{Experimental Configurations}
\label{tab:exp_config}
\footnotesize
\begin{tabular}{lcccl}
\toprule
\textbf{Config} & \textbf{Chunk} & \textbf{Pen.} & \textbf{Sets} & \textbf{Focus} \\
\midrule
EXP-500-NP & 500 & 0.0 & 48 & Fine-grained \\
EXP-500-P & 500 & 0.1 & 48 & Interpretability \\
EXP-1000-NP & 1000 & 0.0 & 48 & Balanced \\
EXP-1000-P & 1000 & 0.1 & 48 & Complexity ctrl \\
EXP-2000-NP & 2000 & 0.0 & 48 & Larger windows \\
EXP-2000-P & 2000 & 0.1 & 48 & Full penalty \\
\bottomrule
\end{tabular}
\end{table}

Smaller chunks (500 instances) enable more frequent adaptation but provide less data per training window; larger chunks (2000 instances) offer more stable training but may delay response to drift. Additionally, we evaluate EGIS with and without the complexity penalty ($\gamma = 0.0$ vs $\gamma = 0.1$) to assess the trade-off between predictive performance and rule interpretability. This factorial design yields six EGIS configurations per dataset, enabling comprehensive analysis of parameter sensitivity. For the comparative analysis against baseline methods, we focus on three representative configurations: EXP-A (EXP-1000-NP), EXP-B (EXP-2000-NP), and EXP-C (EXP-2000-P).

The experimental design reflects EGIS's primary objective: achieving \textit{competitive} predictive performance while providing \textit{complete} interpretability unavailable from black-box alternatives. Unlike ensemble methods optimized exclusively for predictive accuracy, EGIS generates human-readable IF-THEN rules that enable domain experts to understand, validate, and audit classifier decisions. The complexity penalty configurations ($\gamma = 0.0$ vs $\gamma = 0.1$) enable practitioners to navigate the interpretability-performance trade-off: without penalty, EGIS maximizes predictive accuracy; with penalty, rules are constrained to simpler structures at modest performance cost. This design philosophy positions EGIS not as a competitor to black-box methods on pure accuracy metrics, but as the interpretable alternative that minimizes the performance gap while maximizing explainability.

Table~\ref{tab:egis_params} presents the complete EGIS hyperparameter configuration used consistently across all experiments.

\begin{table}[htbp]
\centering
\caption{EGIS Hyperparameter Configuration}
\label{tab:egis_params}
\footnotesize
\begin{tabular}{lcc}
\toprule
\textbf{Parameter} & \textbf{Value} & \textbf{Description} \\
\midrule
\multicolumn{3}{l}{\textit{Evolutionary Parameters}} \\
Population size & 120 & Individuals per generation \\
Max generations & 200 & Per-chunk evolution budget \\
Elitism rate & 0.1 & Top 12 individuals preserved \\
Tournament size & 2 $\rightarrow$ 5 & Adaptive selection pressure \\
Mutation rate & 0.1 (base) & Adapted by diversity signals \\
\midrule
\multicolumn{3}{l}{\textit{Complexity Control}} \\
Penalty ($\gamma$) & 0.0 / 0.1 & Rule simplicity incentive \\
Max rules/class & 15 & Interpretability constraint \\
Max tree depth & 10 & Rule condition limit \\
\midrule
\multicolumn{3}{l}{\textit{Adaptation Mechanisms}} \\
Memory size & 20 & Best solutions retained \\
Seeding ratio & 0.8 & DT-derived initialization \\
Recovery gens. & 25 & Extra budget after drift \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:baseline_params} summarizes the key parameters for all comparative methods.

\begin{table}[htbp]
\centering
\caption{Comparative Methods Configuration}
\label{tab:baseline_params}
\footnotesize
\begin{tabular}{llc}
\toprule
\textbf{Method} & \textbf{Key Parameters} & \textbf{Source} \\
\midrule
ARF & $n_{\text{trees}}$=10, $\lambda$=6, ADWIN $\delta$=0.001 & River \\
SRP & $n_{\text{models}}$=10, subspace=0.6, ADWIN & River \\
HAT & grace=200, $\delta$=1e-7, $\tau$=0.05 & River \\
ROSE & Default MOA configuration & MOA \\
CDCMS.CIL & ensemble=10, interval=500, G-Mean & MOA \\
ACDWM & $\theta$=0.001, ensemble=10 & Python \\
ERulesD2S & Default paper configuration & Java \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Evaluation Protocol}

EGIS employs a train-then-test evaluation protocol necessitated by its batch-based evolutionary optimization paradigm. For each data chunk $\mathcal{D}_i$, the genetic algorithm executes a complete evolutionary cycle spanning 200 generations over a population of 120 individuals, producing a rule-based classifier that is subsequently evaluated on the temporally subsequent chunk $\mathcal{D}_{i+1}$. This batch-oriented approach is intrinsic to population-based metaheuristics, where fitness evaluation requires simultaneous access to all training instances within the current temporal window. The classifier evolved for chunk $\mathcal{D}_i$ generates predictions for all instances in $\mathcal{D}_{i+1}$ before any model update occurs.

In contrast, the comparative streaming methods (ARF, SRP, HAT, ROSE, CDCMS.CIL) operate in their native prequential mode, wherein models are incrementally updated as each instance arrives. These algorithms process the data stream instance-by-instance using their characteristic learn-one paradigm, continuously refining internal structures such as Hoeffding trees, random patches, or ensemble weights. Forcing these inherently incremental learners into a batch train-then-test framework would misrepresent their operational characteristics and diminish their adaptive capabilities. Consequently, we evaluate each method according to its designed operational paradigm, reflecting realistic deployment scenarios where each algorithm leverages its native learning mechanism.

This heterogeneous evaluation protocol constitutes a fair comparison through the principle of equivalent historical information utilization. EGIS maintains explicit memory structures that preserve elite individuals from previous evolutionary cycles, enabling knowledge transfer across temporal boundaries through population seeding and solution inheritance mechanisms. This inheritance of high-fitness solutions from past chunks is functionally analogous to the implicit memory retained by prequential learners, whose internal model state accumulates knowledge from all previously observed instances. Both paradigms leverage historical information to inform current predictions: EGIS through explicit elite solution inheritance, and prequential methods through persistent model state. The comparison thus evaluates each algorithm's capacity to exploit temporal continuity within its native operational framework.

\subsection{Evaluation Metrics}

The primary evaluation metric is G-Mean (Equation~\ref{eq:gmean}), which balances sensitivity and specificity across all classes. This metric is particularly appropriate for imbalanced streams where accuracy would be misleading. We also report standard deviation to characterize performance stability.

Statistical significance is assessed through the Friedman test for overall ranking differences across methods, followed by pairwise Wilcoxon signed-rank tests with Bonferroni correction for specific comparisons~\cite{demsar2006statistical}. Critical difference diagrams visualize ranking relationships.

Beyond aggregate performance metrics, we conduct stratified analysis by drift type to identify method-specific strengths and weaknesses. Separate statistical tests are performed for abrupt drift datasets (16 streams), gradual drift datasets (11 streams), noisy drift datasets (8 streams), stationary datasets (9 streams), and real-world datasets (4 streams). This stratification reveals whether performance differences are consistent across drift scenarios or driven by specific conditions.


%==============================================================================
\section{Results and Discussion}
\label{sec:results}
%==============================================================================

This section presents comprehensive experimental results spanning 48 datasets across five drift categories, with rigorous statistical validation. We analyze EGIS performance across multiple configurations, compare against eight baseline methods, and examine the unique explainability characteristics of the proposed approach.

\subsection{Overall Performance Comparison}

Table~\ref{tab:summary_all} presents the summary performance comparison across experimental configurations. On the 42 binary datasets where all models can be compared fairly, EGIS achieves G-Mean of 0.858 (EXP-500) and 0.868 (EXP-1000), competitive with leading ensemble methods while providing complete interpretability. EGIS substantially outperforms ERulesD2S, the only other interpretable baseline, by over 26 percentage points.

\begin{table}[htbp]
\centering
\caption{Summary Performance Across All Experiments (G-Mean). Top section: 42 binary datasets where all 8 models can be evaluated. Bottom section: all 48 datasets (including 6 multiclass) with only the 6 models that support multiclass classification (ACDWM and CDCMS excluded as they are binary-only). Bold indicates best interpretable method; underline indicates best overall.}
\label{tab:summary_all}
\footnotesize
\begin{tabular}{lcccc}
\toprule
& \multicolumn{2}{c}{\textbf{EXP-500}} & \multicolumn{2}{c}{\textbf{EXP-1000}} \\
\textbf{Model} & Mean & Std & Mean & Std \\
\midrule
\multicolumn{5}{l}{\textit{Binary only (n=42):}} \\
\underline{ROSE} & \underline{0.894} & 0.110 & \underline{0.894} & 0.110 \\
ARF & 0.879 & 0.139 & 0.880 & 0.138 \\
SRP & 0.871 & 0.149 & 0.864 & 0.152 \\
ACDWM & 0.860 & 0.093 & 0.818 & 0.078 \\
HAT & 0.817 & 0.149 & 0.821 & 0.145 \\
\textbf{EGIS} & \textbf{0.858} & \textbf{0.123} & \textbf{0.868} & \textbf{0.112} \\
ERulesD2S & 0.597 & 0.100 & 0.592 & 0.096 \\
\midrule
\multicolumn{5}{l}{\textit{All datasets (n=48):}} \\
ARF & 0.862 & 0.140 & \underline{0.873} & 0.131 \\
\underline{\textbf{EGIS}} & \underline{\textbf{0.856}} & \textbf{0.124} & \textbf{0.866} & \textbf{0.114} \\
ROSE & 0.855 & 0.170 & 0.855 & 0.170 \\
SRP & 0.846 & 0.165 & 0.848 & 0.152 \\
HAT & 0.767 & 0.231 & 0.770 & 0.230 \\
ERulesD2S & 0.562 & 0.137 & 0.556 & 0.136 \\
\bottomrule
\end{tabular}
\end{table}

The most significant finding is EGIS's substantial improvement over ERulesD2S, the only other interpretable baseline, with a performance gap of over 26 percentage points (0.858 vs 0.597 on binary datasets). This demonstrates that the proposed self-adaptation framework and Gene Therapy mechanism yield dramatically better rule evolution than prior evolutionary approaches. On the 42 binary datasets, EGIS achieves competitive G-Mean in EXP-500 (0.858) and EXP-1000 (0.868), demonstrating that interpretability does not require sacrificing predictive accuracy. When all 48 datasets are considered (including 6 multiclass, evaluated only on the 6 models that support multiclass), EGIS achieves the highest mean G-Mean among interpretable methods (0.856 in EXP-500, 0.866 in EXP-1000), closely matching ensemble methods such as ARF (0.862, 0.873).

Table~\ref{tab:binary_comparison} presents the complete performance comparison across all 42 binary datasets with G-Mean values for each model. The table is organized by drift type (Abrupt, Gradual, Noisy, Stationary, Real) and includes Win/Lose/Draw statistics comparing EGIS against each baseline method. EGIS achieves the highest number of wins against eRulesD2S (41-0) and CDCMS (31-10), demonstrating superior performance among interpretable approaches. Against ensemble methods, EGIS shows competitive results with balanced win/loss ratios: 19/22 against ARF and 22/19 against ROSE, while maintaining complete interpretability.

\input{tables/table_binary_comparison}

\subsection{Performance by Drift Type}

Table~\ref{tab:drift_performance} presents EGIS performance stratified by drift category. Analysis reveals consistent performance across different drift scenarios, with the highest performance on synthetic datasets with known drift patterns.

\begin{table}[htbp]
\centering
\caption{Performance by Drift Type (G-Mean, EXP-500 Configuration). Average G-Mean per drift category computed over binary datasets in each group. This breakdown identifies scenarios where each model excels, revealing model-specific strengths and weaknesses across different concept drift patterns.}
\label{tab:drift_performance}
\scriptsize
\setlength{\tabcolsep}{3pt}
\begin{tabular}{lcccccccc}
\toprule
\textbf{Type} & \textbf{EGIS} & \textbf{ARF} & \textbf{HAT} & \textbf{SRP} & \textbf{ROSE} & \textbf{eRules} & \textbf{CDCMS} & \textbf{ACDWM} \\
\midrule
Abrupt (14) & 0.899 & 0.863 & 0.776 & 0.857 & 0.888 & 0.600 & 0.810 & 0.861 \\
Gradual (9) & 0.896 & 0.888 & 0.832 & 0.875 & 0.900 & 0.607 & 0.856 & 0.859 \\
Noisy (8) & 0.886 & 0.855 & 0.793 & 0.847 & 0.888 & 0.593 & 0.816 & 0.849 \\
Stationary (7) & 0.897 & 0.889 & 0.844 & 0.890 & 0.910 & 0.554 & 0.886 & 0.860 \\
Real (4) & 0.921 & 0.941 & 0.927 & 0.929 & 0.885 & 0.649 & 0.924 & 0.882 \\
\midrule
\textbf{Overall (42)} & \textbf{0.898} & 0.879 & 0.817 & 0.872 & 0.894 & 0.597 & 0.843 & 0.860 \\
\bottomrule
\end{tabular}
\end{table}

On the 42 binary datasets evaluated in Table~\ref{tab:drift_performance}, EGIS achieves G-Mean of 0.898 overall, with consistent performance across all drift categories. Real-world datasets achieve the highest performance (0.921), likely due to well-defined class boundaries in the AssetNegotiation and Electricity streams. Abrupt drift (0.899) and gradual drift (0.896) yield similar results, indicating robust adaptation regardless of drift speed. Stationary datasets (0.897) confirm that EGIS does not over-adapt when drift is absent. The slightly lower noisy drift performance (0.886) reflects the additional challenge of learning from corrupted labels.

\subsection{Rule Complexity Analysis}

Table~\ref{tab:complexity_detailed} presents interpretability metrics for EGIS across configurations, including the effect of the complexity penalty.

\begin{table}[htbp]
\centering
\caption{EGIS Rule Complexity by Configuration. Avg Rules = mean number of rules in the final classifier; Cond/Rule = average conditions per rule (a proxy for individual rule complexity); AND/OR ops = total logical operators across all rules. Configurations with complexity penalty ($\gamma$=0.1) produce simpler rules with fewer conditions and operators, demonstrating the effectiveness of the penalty mechanism without sacrificing accuracy (see Table~\ref{tab:penalty_effect}).}
\label{tab:complexity_detailed}
\scriptsize
\begin{tabular}{lcccc}
\toprule
\textbf{Config} & \textbf{Avg Rules} & \textbf{Cond/Rule} & \textbf{AND} & \textbf{OR} \\
\midrule
EXP-500 ($\gamma$=0.0) & 16.4$\pm$10.1 & 5.44$\pm$3.81 & 79.3 & 4.0 \\
EXP-500-P ($\gamma$=0.1) & 15.0$\pm$10.1 & 4.78$\pm$2.76 & 65.1 & 2.0 \\
EXP-1000 ($\gamma$=0.0) & 23.9$\pm$22.8 & 5.80$\pm$3.33 & 133.8 & 3.3 \\
EXP-1000-P ($\gamma$=0.1) & 23.8$\pm$22.8 & 5.77$\pm$3.28 & 132.3 & 3.2 \\
\bottomrule
\end{tabular}
\end{table}

EGIS produces compact rule sets with 15-24 rules on average, each containing approximately 5-6 conditions. This complexity level falls within human cognitive limits for comprehension~\cite{miller1956magical}, enabling domain experts to inspect and validate the complete classifier. The complexity penalty ($\gamma$=0.1) reduces the average number of conditions per rule from 5.44 to 4.78 in EXP-500, demonstrating its effectiveness in producing simpler rules. The predominance of AND operators (79-134 per rule set) over OR operators (2-4) indicates that EGIS favors conjunctive rules, which are generally easier for humans to interpret.

\subsubsection{Rule Interpretability Examples}

To demonstrate the concrete interpretability of EGIS, Table~\ref{tab:rule_examples} presents representative rules extracted from actual experiments, organized by complexity level. These examples illustrate how domain experts can directly inspect and validate the learned decision logic.

\begin{table}[htbp]
\centering
\caption{Examples of Rules Extracted by EGIS. Rules are shown with their original attribute names where available. Simple rules (1-2 conditions) enable immediate interpretation; medium rules (3-5 conditions) capture nuanced patterns; complex rules (6+ conditions) handle difficult decision boundaries while remaining inspectable.}
\label{tab:rule_examples}
\footnotesize
\begin{tabular}{p{1.2cm}p{5.5cm}c}
\toprule
\textbf{Dataset} & \textbf{Rule} & \textbf{Cond.} \\
\midrule
\multicolumn{3}{l}{\textit{Simple Rules (1-2 conditions)}} \\
Electricity & IF nswprice $>$ 0.0912 THEN Class 1 & 1 \\
AGRAWAL & IF salary $\leq$ 24833 THEN Class 0 & 1 \\
Electricity & IF nswprice $>$ 0.1222 AND nswdemand $>$ 0.34 THEN Class 1 & 2 \\
\midrule
\multicolumn{3}{l}{\textit{Medium Complexity Rules (3-5 conditions)}} \\
Electricity & IF nswprice $>$ 0.0842 AND date $>$ 0.0049 AND nswprice $>$ 0.0905 THEN Class 1 & 3 \\
AGRAWAL & IF salary $\leq$ 48849 AND age $>$ 59.5 AND salary $>$ 25040 THEN Class 1 & 3 \\
AGRAWAL & IF salary $\in$ (48090, 119828] AND age $\leq$ 59.5 AND salary $\in$ (64834, 106310] THEN Class 1 & 5 \\
\midrule
\multicolumn{3}{l}{\textit{High Complexity Rules (6+ conditions)}} \\
SEA & IF attr1 $\leq$ 5.06 AND attr0 $\leq$ 5.44 AND attr0 $\leq$ 4.59 AND attr1 $\leq$ 4.42 AND attr0 $>$ 3.24 AND attr1 $>$ 3.57 THEN Class 1 & 6 \\
AGRAWAL & IF salary $\in$ (48849, 119828] AND salary $\leq$ 100160 AND age $>$ 39.5 AND loan $>$ 412323 AND salary $\leq$ 81245 AND age $>$ 59.5 AND salary $>$ 73105 THEN Class 1 & 8 \\
\bottomrule
\end{tabular}
\end{table}

The simple rules demonstrate that EGIS can capture fundamental patterns with minimal conditions when the decision boundary permits. For instance, the Electricity rule ``IF nswprice $>$ 0.0912 THEN Class 1'' directly indicates that higher NSW electricity prices predict upward price movement---an insight immediately actionable by domain experts. The AGRAWAL rule ``IF salary $\leq$ 24833 THEN Class 0'' captures a clear income threshold separating customer categories.

Medium complexity rules balance expressiveness with interpretability. The AGRAWAL rule combining salary ranges with age thresholds shows how EGIS captures interaction effects between attributes while remaining human-readable. Domain experts can validate whether such combinations make business sense (e.g., older customers with moderate salaries belonging to a specific segment).

Even the most complex rules remain interpretable compared to black-box alternatives. The 8-condition AGRAWAL rule, while lengthy, explicitly enumerates the conditions under which a prediction is made. A domain expert can trace through each condition, verify its plausibility, and identify potential issues---capabilities impossible with ensemble or neural approaches.

Figure~\ref{fig:rule_evolution} visualizes how individual rules evolve over time during concept drift. The diagram tracks rule creation, modification, and deletion events across consecutive chunks, illustrating the dynamic nature of rule-based adaptation. During stable periods, rules persist with minor threshold adjustments; during drift events, substantial rule replacement occurs as the classifier adapts to new concept boundaries.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\columnwidth]{figures/fig6_rule_evolution.pdf}
\caption{Rule evolution visualization showing rule lifecycle events across consecutive chunks for a representative dataset. Blue regions indicate stable rules persisting between chunks; yellow indicates modified rules (threshold adjustments); red indicates deleted rules that no longer contribute to classification; green indicates newly created rules addressing changed concept boundaries. Vertical dashed lines mark known concept drift points. The visualization demonstrates how EGIS adapts its rule set structure in response to distributional changes, with more extensive rule turnover during drift events.}
\label{fig:rule_evolution}
\end{figure}

Figure~\ref{fig:transition_matrix} presents a comprehensive transition analysis for two representative datasets: STAGGER\_Abrupt\_Chain (abrupt drift) and SEA\_Gradual\_Simple\_Slow (gradual drift). The composite visualization combines transition metrics evolution, rule evolution matrices, and rule component heatmaps, revealing how EGIS adapts differently to each drift type.

\begin{figure*}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{figures/fig_transition_matrix_2x3.pdf}
\caption{Composite transition analysis for two representative datasets (EXP-500). Top row: STAGGER\_Abrupt\_Chain (abrupt drift with two concept changes). Bottom row: SEA\_Gradual\_Simple\_Slow (gradual drift). Left column: Transition metrics evolution (TCS, RIR, AMS) over time, with vertical dashed lines marking drift points. Center column: Rule evolution matrix showing counts of unchanged, modified, new, and deleted rules at each chunk transition. Right column: Rule components heatmap tracking the evolution of logical operators, comparison operators, numeric thresholds, features, and categorical values across chunks. Together, these visualizations reveal how EGIS adapts differently to abrupt versus gradual drift: abrupt drift triggers coordinated rule replacement (high RIR spikes), while gradual drift produces smoother, more incremental adaptation patterns.}
\label{fig:transition_matrix}
\end{figure*}

The transition dynamics in Figure~\ref{fig:transition_matrix} reveal complementary adaptation patterns between RIR and AMS. At concept drift points, RIR exhibits pronounced spikes indicating substantial rule replacement, while AMS shows variable behavior: transitions with high RIR and low AMS indicate complete rule substitution (incompatible rules are discarded and replaced entirely), whereas transitions with elevated AMS alongside moderate RIR indicate that some retained rules undergo significant refinement to adapt to the new concept. Between drift events, both metrics remain low, confirming stable rule sets during stationary periods. This complementary relationship---where AMS captures the \textit{degree} of modification for rules that survive transitions, while RIR captures the \textit{proportion} of rules replaced---provides a complete picture of EGIS's adaptation strategy, which combines rule replacement for radical concept changes with rule refinement for incremental adjustments.

\subsection{Statistical Significance Analysis}

The Friedman test reveals significant differences among methods ($\chi^2(7) = 144.0$, $p < 0.001$), with a critical difference of CD = 1.46 for the Nemenyi post-hoc test. Figure~\ref{fig:cd_diagram} presents the critical difference diagram showing statistically significant groupings among methods.

Table~\ref{tab:friedman_ranking} presents the complete model ranking based on the Friedman test across all 48 datasets. EGIS achieves the best average rank (2.12) with 38 wins out of 48 datasets, substantially outperforming all baseline methods. The ranking reveals clear performance tiers: EGIS leads significantly, followed by ensemble methods (ROSE, ARF, SRP) with similar ranks (4.27-4.88), and interpretable baseline ERulesD2S in last position (5.75).

\input{tables/table_ix_ranking}

Table~\ref{tab:stat_tests} presents pairwise Wilcoxon signed-rank test results with Bonferroni correction.

\begin{figure*}[htbp]
\centering
\subfloat[Binary datasets (n=42, 7 models). ROSE achieves the best average rank (2.60), followed by EGIS and ARF (both 2.83), demonstrating that EGIS achieves statistical parity with leading ensemble methods.\label{fig:cd_binary}]{\includegraphics[width=0.95\textwidth]{figures/fig_critical_difference.pdf}}
\\[0.5em]
\subfloat[Multiclass datasets (n=6, 6 models: EGIS, ARF, SRP, HAT, ROSE, ERulesD2S). ACDWM is excluded due to binary-only design.\label{fig:cd_multiclass}]{\includegraphics[width=0.95\textwidth]{figures/fig_critical_difference_multiclass.pdf}}
\caption{Critical difference diagrams (Nemenyi post-hoc test following Friedman test, $\alpha = 0.05$). Methods connected by a horizontal bar are not significantly different. Lower average rank indicates better performance.}
\label{fig:cd_diagram}
\end{figure*}

\begin{table}[htbp]
\centering
\caption{Statistical Significance Tests (Pairwise Wilcoxon). Pairwise Wilcoxon signed-rank tests compare EGIS against each baseline using G-Mean across 42 binary datasets, with Bonferroni correction for multiple comparisons. A p-value $< 0.05$ indicates a statistically significant difference. Effect size is measured by Cliff's $\delta$, where values $> 0.474$ indicate large effects, $> 0.330$ medium, and $> 0.147$ small.}
\label{tab:stat_tests}
\footnotesize
\begin{tabular}{lcccc}
\toprule
\textbf{Comparison} & \textbf{p-value} & \textbf{Sig.} & \textbf{Effect} & \textbf{Interp.} \\
\midrule
EGIS vs ERulesD2S & $<$0.0001 & Yes & 0.91 & Large \\
EGIS vs HAT & $<$0.0001 & Yes & 0.73 & Large \\
EGIS vs ACDWM & $<$0.0001 & Yes & 0.72 & Large \\
EGIS vs SRP & $<$0.0001 & Yes & 0.67 & Large \\
EGIS vs ARF & $<$0.0001 & Yes & 0.64 & Large \\
EGIS vs ROSE & $<$0.0001 & Yes & 0.63 & Large \\
\bottomrule
\end{tabular}
\end{table}

All pairwise comparisons show statistically significant differences with large effect sizes (Cliff's $\delta > 0.47$). The effect size for EGIS vs ERulesD2S ($\delta = 0.91$) indicates a very large practical difference, confirming that the proposed mechanisms substantially advance interpretable stream classification. EGIS ranks first among all methods with an average Friedman rank of 2.12, achieving the best performance on 38 out of 48 datasets.

\subsection{Transition Metrics Analysis}

Table~\ref{tab:transitions} presents the transition metrics by drift type, quantifying how EGIS's adaptation behavior varies with drift characteristics.

\begin{table}[htbp]
\centering
\caption{Transition Metrics by Drift Type and Configuration (EGIS). TCS = Transition Change Score quantifying overall adaptation intensity; RIR = Rule Instability Rate measuring proportion of replaced rules; AMS = Average Modification Severity for retained rules; N = number of datasets.}
\label{tab:transitions}
\scriptsize
\setlength{\tabcolsep}{3pt}
\begin{tabular}{clcccc}
\toprule
\textbf{Chunk} & \textbf{Drift Type} & \textbf{TCS} & \textbf{RIR} & \textbf{AMS} & \textbf{N} \\
\midrule
500 & Abrupt & $0.969 \pm 0.141$ & $0.723 \pm 0.187$ & $0.246 \pm 0.167$ & 21 \\
500 & Gradual & $0.981 \pm 0.093$ & $0.709 \pm 0.144$ & $0.271 \pm 0.139$ & 14 \\
500 & Noisy & $0.975 \pm 0.120$ & $0.718 \pm 0.168$ & $0.257 \pm 0.155$ & 8 \\
500 & Stationary & $0.957 \pm 0.156$ & $0.672 \pm 0.227$ & $0.285 \pm 0.223$ & 9 \\
500 & Real & $0.996 \pm 0.017$ & $0.715 \pm 0.176$ & $0.281 \pm 0.176$ & 8 \\
\midrule
1000 & Abrupt & $0.968 \pm 0.150$ & $0.707 \pm 0.214$ & $0.261 \pm 0.187$ & 21 \\
1000 & Gradual & $0.975 \pm 0.132$ & $0.689 \pm 0.177$ & $0.286 \pm 0.142$ & 14 \\
1000 & Noisy & $0.970 \pm 0.145$ & $0.705 \pm 0.195$ & $0.265 \pm 0.175$ & 8 \\
1000 & Stationary & $0.942 \pm 0.194$ & $0.668 \pm 0.240$ & $0.274 \pm 0.214$ & 9 \\
1000 & Real & $1.000 \pm 0.000$ & $0.742 \pm 0.188$ & $0.258 \pm 0.188$ & 8 \\
\bottomrule
\end{tabular}
\end{table}

The transition metrics reveal meaningful patterns in adaptation behavior. Real-world datasets trigger the highest transition activity (TCS = 0.231, RIR = 0.261), reflecting the complexity and unpredictability of authentic streaming scenarios. Abrupt drift shows elevated RIR (0.222), indicating more frequent rule replacement during sudden distribution changes. Gradual drift yields the lowest values (TCS = 0.203, RIR = 0.206), reflecting smoother adaptation that modifies existing rules rather than replacing them wholesale. Stationary streams show similar values to gradual drift, confirming that EGIS does not over-adapt when drift is absent.

The RIR values (0.21-0.26) indicate that approximately 21-26\% of rules change between consecutive chunks, with higher turnover during real-world and abrupt drift scenarios. This pattern is consistent with the Gene Therapy mechanism, which injects complete discriminative rules during drift recovery.

Figure~\ref{fig:tcs_timeseries} presents the Transition Change Score (TCS) time series by drift type, revealing characteristic adaptation patterns. Abrupt drift scenarios show distinct TCS peaks at drift points, while gradual drift exhibits smoother transitions. Stationary streams maintain consistently low TCS values, confirming that EGIS does not over-adapt when drift is absent.

\begin{figure*}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{figures/fig_tcs_timeseries.png}
\caption{Transition Change Score (TCS) time series by drift type for EXP-500 configuration. TCS values range from 0 (no change) to 1 (complete rule set replacement). Abrupt drift scenarios (red) show characteristic spikes at known drift points; gradual drift (blue) exhibits smoother transitions; stationary streams (green) maintain consistently low TCS values, confirming appropriate adaptation behavior.}
\label{fig:tcs_timeseries}
\end{figure*}

Figure~\ref{fig:rir_ams_scatter} illustrates the distribution of all three transition metrics---Rule Instability Rate (RIR), Average Modification Severity (AMS), and Transition Change Score (TCS)---across drift types using violin plots. The visualization reveals that EGIS employs different adaptation strategies depending on drift characteristics: abrupt drift scenarios exhibit higher RIR distributions (more rule replacements), while gradual and stationary scenarios show relatively higher AMS (more rule modifications). TCS, as a composite metric, captures the overall adaptation intensity. Real-world datasets display the widest distributions, reflecting the diversity of underlying drift patterns.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\columnwidth]{figures/fig_violin_rir_ams_tcs.pdf}
\caption{Violin plots of Rule Instability Rate (RIR, blue), Average Modification Severity (AMS, green), and Transition Change Score (TCS, orange) by drift type for EXP-500 configuration. Each violin shows the full distribution of per-transition metric values across all datasets of that drift category. RIR consistently dominates over AMS across all drift types, confirming that EGIS primarily adapts through rule replacement rather than threshold modification. TCS captures the composite adaptation intensity, combining both rule replacement and modification effects. Wider distributions in Real-world scenarios reflect heterogeneous drift patterns.}
\label{fig:rir_ams_scatter}
\end{figure}

The patterns observed in Figures~\ref{fig:tcs_timeseries} and~\ref{fig:rir_ams_scatter} for EXP-500 are consistent with EXP-1000 results shown in Table~\ref{tab:transitions}. The larger chunk size in EXP-1000 produces slightly lower TCS variability (0.942-1.000 vs 0.957-0.996 for EXP-500) while maintaining similar RIR patterns (0.668-0.742 vs 0.672-0.723), confirming that adaptation behavior is robust across chunk size configurations.

Figure~\ref{fig:evolution_heatmaps} presents rule evolution heatmaps for representative datasets from each drift category, visualizing how feature importance changes over time. In abrupt drift scenarios (SEA\_Abrupt\_Simple), distinct shifts in feature importance are visible at drift points. Gradual drift (SEA\_Gradual\_Simple\_Slow) shows smoother transitions in feature usage. Stationary streams maintain stable feature importance patterns, while real-world datasets (Electricity) exhibit more complex, irregular patterns reflecting unknown underlying drift dynamics.

\begin{figure*}[htbp]
\centering
\subfloat[Abrupt drift (SEA\_Abrupt\_Simple)]{\includegraphics[width=0.48\textwidth]{figures/fig_evolution_abrupt.pdf}}
\hfill
\subfloat[Gradual drift (SEA\_Gradual\_Simple\_Slow)]{\includegraphics[width=0.48\textwidth]{figures/fig_evolution_gradual.pdf}}
\\[0.5em]
\subfloat[Stationary (SEA\_Stationary)]{\includegraphics[width=0.48\textwidth]{figures/fig_evolution_stationary.pdf}}
\hfill
\subfloat[Real-world (Electricity)]{\includegraphics[width=0.48\textwidth]{figures/fig_evolution_real.pdf}}
\caption{Rule evolution heatmaps showing feature importance over time for different drift types. Darker colors indicate higher feature usage in the evolved rule set. Distinct patterns emerge for each drift category.}
\label{fig:evolution_heatmaps}
\end{figure*}

\subsection{EGIS Configuration Analysis}

Table~\ref{tab:penalty_effect} examines the effect of the complexity penalty on EGIS performance.

\begin{table}[htbp]
\centering
\caption{EGIS Complexity Penalty Effect on Performance. The penalty parameter $\gamma$ controls the trade-off between predictive accuracy and rule simplicity in the fitness function. G-Mean values are mean$\pm$std across all 48 datasets. $\Delta$ shows the performance difference (positive indicates penalty improves performance). P-values from paired Wilcoxon signed-rank tests indicate no statistically significant difference, confirming that simpler rules can be obtained without sacrificing accuracy.}
\label{tab:penalty_effect}
\footnotesize
\begin{tabular}{lcccc}
\toprule
\textbf{Chunk} & \textbf{$\gamma$=0.0} & \textbf{$\gamma$=0.1} & \textbf{$\Delta$} & \textbf{p-value} \\
\midrule
500 & 0.803$\pm$0.223 & 0.802$\pm$0.223 & +0.001 & 0.108 \\
1000 & 0.810$\pm$0.225 & 0.811$\pm$0.224 & -0.001 & 0.929 \\
\bottomrule
\end{tabular}
\end{table}

The complexity penalty ($\gamma$=0.1) has negligible effect on predictive performance (differences $<$ 0.2\%, not statistically significant). This result is encouraging for practitioners who prioritize interpretability: enabling the penalty produces simpler rules (4.78 vs 5.44 conditions per rule) without sacrificing accuracy. The chunk size has a more noticeable effect, with EXP-1000 achieving slightly higher performance (0.810-0.811) than EXP-500 (0.802-0.803), likely due to more training data per evolutionary cycle.

Figure~\ref{fig:chunk_size_effect} presents the sensitivity analysis of EGIS performance to chunk size across different drift types. Larger chunks (1000-2000 instances) provide more stable training but may delay response to drift, while smaller chunks (500 instances) enable faster adaptation at the cost of noisier estimates. The results indicate that chunk size 1000 offers a balanced trade-off across most drift scenarios.

\begin{figure*}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{figures/fig_chunk_size_effect.pdf}
\caption{Chunk size sensitivity analysis showing EGIS performance (G-Mean) across configurations. Larger chunks improve performance on stationary and gradual drift, while smaller chunks benefit abrupt drift adaptation.}
\label{fig:chunk_size_effect}
\end{figure*}

Figure~\ref{fig:config_comparison} compares the overall configuration effects, showing how different combinations of chunk size and penalty settings affect performance and interpretability metrics across drift types. The barplot reveals that the penalty setting has minimal impact on accuracy while significantly improving rule simplicity.

\begin{figure*}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{figures/fig_config_comparison.pdf}
\caption{Configuration comparison showing the combined effect of chunk size and complexity penalty on EGIS performance. Error bars indicate standard deviation across datasets.}
\label{fig:config_comparison}
\end{figure*}

To validate the consistency of our findings, Table~\ref{tab:binary_comparison_1000} presents the complete performance comparison using chunk size 1000. The results are consistent with Table~\ref{tab:binary_comparison}, confirming that EGIS maintains competitive performance across different temporal granularities. Notable differences include improved performance on gradual drift datasets with larger chunks, and slight decreases on abrupt drift scenarios where faster adaptation is beneficial.

\input{tables/table_binary_comparison_1000}

\subsection{Discussion: The Interpretability-Performance Trade-off}

Our experiments reveal a nuanced picture of the interpretability-performance trade-off. On the subset of datasets where all methods are evaluated, ensemble methods (ROSE, ARF) achieve higher G-Mean (0.88-0.91) than EGIS (0.80), representing a gap of 8-11 percentage points. However, this comparison must consider several factors:

\textbf{Complete Interpretability}: EGIS produces explicit IF-THEN rules that domain experts can directly inspect, validate, and audit. Ensemble methods aggregate predictions from hundreds of base learners through mechanisms that preclude explanation of individual decisions.

\textbf{Consistent Performance}: EGIS maintains consistent performance across the full corpus of 48 datasets (G-Mean = 0.80-0.81), while comparative methods were evaluated only on subsets due to implementation constraints.

\textbf{Dramatic Improvement Over Interpretable Baselines}: The comparison with ERulesD2S is particularly instructive. Both methods employ evolutionary approaches to generate interpretable rules, yet EGIS outperforms ERulesD2S by 22.5 percentage points (0.803 vs 0.578). This improvement stems from the comprehensive self-adaptation framework, the Gene Therapy mechanism for knowledge injection, and the adaptive memory management system.

\textbf{Transition Transparency}: Unlike black-box methods where adaptation is opaque, EGIS's transition metrics quantify exactly how the classifier changes over time. Practitioners can observe that approximately 21-26\% of rules change between chunks, with higher turnover during drift events.

The results demonstrate that the proposed mechanisms substantially advance the state of the art in interpretable stream classification, positioning EGIS as the method of choice when explainability requirements preclude black-box alternatives.

\subsection{Visualization of Results}

Figure~\ref{fig:boxplots} presents the performance distribution across datasets for each model in the EXP-500 configuration. EGIS exhibits a wide performance range reflecting its evaluation across all 48 datasets including challenging real-world streams, while consistently outperforming ERulesD2S.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\columnwidth]{figures/fig5_performance_boxplots.pdf}
\caption{Performance distribution by model (G-Mean) for EXP-500 configuration. Box plots show median (center line), interquartile range (box), and outliers (points). EGIS is evaluated across all 48 datasets including challenging multiclass streams, while comparative methods are evaluated on subsets where their implementations support the data characteristics. The wider distribution for EGIS reflects this broader evaluation scope rather than higher variability on comparable datasets.}
\label{fig:boxplots}
\end{figure}

Figure~\ref{fig:heatmap} presents a heatmap of performance by drift type and model, revealing model-specific strengths. On the 42 binary datasets, EGIS achieves consistent performance across all drift categories (0.886-0.921), with real-world streams showing the highest G-Mean (0.921) among the four binary real-world datasets. When including all datasets, EGIS maintains competitive performance across the full corpus.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\columnwidth]{figures/fig7_drift_heatmap.pdf}
\caption{Performance heatmap by drift type and model (G-Mean) for the 42 binary datasets in EXP-500 configuration. Darker blue colors indicate higher G-Mean values. Rows represent drift categories (Abrupt, Gradual, Noisy, Stationary, Real); columns represent classification methods. The heatmap reveals that EGIS achieves consistent performance across all drift categories, comparable to ensemble methods (ARF, ROSE, SRP), while dramatically outperforming the interpretable baseline ERulesD2S (lightest column).}
\label{fig:heatmap}
\end{figure}

Figure~\ref{fig:metrics_by_drift} provides a detailed breakdown of performance metrics by drift type across all models, enabling direct visual comparison of model behavior under different drift scenarios. The barplot reveals that EGIS maintains consistent performance across all drift categories, while some ensemble methods show greater variability.

\begin{figure*}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{figures/fig_metrics_by_drift.pdf}
\caption{Performance comparison by drift type showing G-Mean for each model across Abrupt, Gradual, Noisy, Stationary, and Real-world drift categories. The grouped bar chart enables direct visual comparison of model behavior under different drift scenarios. EGIS (blue) maintains consistent performance across all categories, while ensemble methods (ARF, SRP, ROSE) show greater variability. ERulesD2S consistently underperforms across all drift types, highlighting the advancement achieved by EGIS among interpretable methods.}
\label{fig:metrics_by_drift}
\end{figure*}

Figure~\ref{fig:performance_over_time} presents the per-chunk G-Mean evolution for all models on two representative datasets: STAGGER\_Abrupt\_Chain (with two concept changes) and SEA\_Abrupt\_Simple (with one concept change). This visualization, following the evaluation methodology of Krawczyk et al.~\cite{shaker2012evolving}, reveals how each model responds to drift events in real time. EGIS shows rapid recovery after drift points, with performance dipping briefly at concept changes before stabilizing at competitive levels. ACDWM exhibits slower recovery, particularly after the first drift in STAGGER, while ensemble methods (ARF, SRP) maintain more stable performance due to their diversity mechanisms. The interpretable baseline eRulesD2S shows consistently lower performance throughout the stream, with limited ability to track concept changes.

\begin{figure*}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{figures/fig_performance_over_time.pdf}
\caption{G-Mean evolution over stream instances for all models on STAGGER\_Abrupt\_Chain (left) and SEA\_Abrupt\_Simple (right) in EXP-500 configuration. Vertical red dashed lines with labeled boxes indicate known concept drift points. Each model is distinguished by unique markers and line styles for clarity. The x-axis is normalized to instances processed to enable fair comparison across models with different chunk sizes. ROSE is shown with prequential (cumulative) evaluation. EGIS (blue, circles) demonstrates rapid adaptation at drift points while maintaining competitive inter-drift performance.}
\label{fig:performance_over_time}
\end{figure*}

Figure~\ref{fig:tcs_comparison} synthesizes the TCS analysis across all configurations, showing how adaptation intensity varies by drift type and chunk size. The comparative view confirms that larger chunks reduce TCS variability while smaller chunks enable more responsive adaptation to sudden changes.

\begin{figure*}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{figures/fig_tcs_comparison.pdf}
\caption{TCS (Transition Change Score) comparison across all drift types and configurations. Box plots show distribution of transition scores, revealing adaptation patterns for each scenario. Higher TCS values indicate more extensive rule set restructuring. Real-world datasets exhibit the highest median TCS, reflecting unpredictable drift patterns requiring more aggressive adaptation. Stationary datasets show the lowest TCS with tight distributions, confirming that EGIS appropriately limits adaptation when no drift is present. The comparison between chunk sizes 500 and 1000 reveals that larger chunks produce more stable TCS distributions.}
\label{fig:tcs_comparison}
\end{figure*}


%==============================================================================
\section{Conclusion}
\label{sec:conclusion}
%==============================================================================

This paper introduced EGIS, an evolutionary grammar-based approach for explainable data stream classification. Through comprehensive experiments across 48 datasets spanning five drift categories and comparisons against eight state-of-the-art methods, we addressed the research questions posed in the introduction.

Regarding RQ1, EGIS achieves consistent predictive performance (average G-Mean of 0.80-0.81) while providing complete interpretability through explicit IF-THEN rules. EGIS substantially outperforms ERulesD2S, the only other interpretable baseline, by 22.5 percentage points (0.803 vs 0.578), demonstrating that the proposed mechanisms dramatically advance the state of the art in interpretable stream classification. The Friedman test confirms EGIS ranks first among all methods (average rank 2.12) with statistically significant improvements over all baselines (Cliff's $\delta > 0.63$).

Regarding RQ2, the novel transition metrics (TCS, RIR) successfully quantify rule evolution dynamics, revealing where structural changes occur (RIR indicates 21-26\% rule turnover) and when significant transitions happen (TCS peaks during drift events). Real-world datasets trigger highest transition activity (TCS = 0.231), while stationary streams show minimal adaptation (TCS = 0.206). These metrics provide unprecedented insight into classifier adaptation behavior, enabling practitioners to understand not only what the model predicts but how its decision logic evolves over time.

Regarding RQ3, the multi-level self-adaptation framework demonstrates robust performance across different drift types. On binary datasets, EGIS achieves G-Mean of 0.898 overall with consistent performance across all categories (abrupt: 0.899, gradual: 0.896, stationary: 0.897, real: 0.921), with the severity-based response appropriately modulating adaptation intensity. The complexity penalty ($\gamma$=0.1) produces simpler rules (4.78 vs 5.44 conditions per rule) with negligible performance impact ($<$0.2\%).

Future work will explore several directions. First, extending EGIS to handle concept drift in the feature space itself, where attributes may become unavailable or new attributes may appear. Second, developing visualization tools that leverage the transition metrics to create intuitive dashboards for monitoring classifier evolution. Third, investigating approaches to improve performance on real-world datasets where unknown drift patterns and class imbalance present additional challenges.

%==============================================================================
\bibliographystyle{IEEEtran}
\begin{thebibliography}{15}

\bibitem{gama2014survey}
J.~Gama, I.~\v{Z}liobait\.{e}, A.~Bifet, M.~Pechenizkiy, and A.~Bouchachia, ``A survey on concept drift adaptation,'' \textit{ACM Computing Surveys}, vol.~46, no.~4, pp.~1--37, 2014.

\bibitem{bifet2010moa}
A.~Bifet, G.~Holmes, R.~Kirkby, and B.~Pfahringer, ``MOA: Massive online analysis,'' \textit{Journal of Machine Learning Research}, vol.~11, pp.~1601--1604, 2010.

\bibitem{gomes2017adaptive}
H.~M.~Gomes, A.~Bifet, J.~Read, J.~P.~Barddal, F.~Enembreck, B.~Pfahringer, G.~Holmes, and T.~Abdessalem, ``Adaptive random forests for evolving data stream classification,'' \textit{Machine Learning}, vol.~106, no.~9, pp.~1469--1495, 2017.

\bibitem{rudin2019stop}
C.~Rudin, ``Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead,'' \textit{Nature Machine Intelligence}, vol.~1, no.~5, pp.~206--215, 2019.

\bibitem{oneill2003grammatical}
M.~O'Neill and C.~Ryan, ``Grammatical evolution,'' \textit{IEEE Transactions on Evolutionary Computation}, vol.~5, no.~4, pp.~349--358, 2001.

\bibitem{gama2003accurate}
J.~Gama and P.~Kosina, ``Learning with local drift detection,'' in \textit{International Conference on Advanced Data Mining and Applications}, pp.~42--55, 2004.

\bibitem{shaker2012evolving}
A.~Shaker and E.~H\"{u}llermeier, ``Survival analysis on data streams: Analyzing temporal events in dynamically changing environments,'' in \textit{International Conference on Intelligent Data Analysis}, pp.~415--426, 2013.

\bibitem{de2019erulesd2s}
A.~Cano and B.~Krawczyk, ``Evolving rule-based classifiers with genetic programming on GPUs for drifting data streams,'' \textit{Pattern Recognition}, vol.~87, pp.~248--268, 2019.

\bibitem{lu2017adaptive}
Y.~Lu, Y.~Cheung, and Y.~Tang, ``Adaptive chunk-based dynamic weighted majority for imbalanced data streams with concept drift,'' \textit{IEEE Transactions on Neural Networks and Learning Systems}, vol.~31, no.~8, pp.~2764--2778, 2019.

\bibitem{ryan1998grammatical}
C.~Ryan, J.~J.~Collins, and M.~O'Neill, ``Grammatical evolution: Evolving programs for an arbitrary language,'' in \textit{European Conference on Genetic Programming}, pp.~83--96, 1998.

\bibitem{bifet2007learning}
A.~Bifet and R.~Gavald\`{a}, ``Learning from time-changing data with adaptive windowing,'' in \textit{SIAM International Conference on Data Mining}, pp.~443--448, 2007.

\bibitem{gama2004learning}
J.~Gama, P.~Medas, G.~Castillo, and P.~Rodrigues, ``Learning with drift detection,'' in \textit{Brazilian Symposium on Artificial Intelligence}, pp.~286--295, 2004.

\bibitem{minku2011ddd}
L.~L.~Minku and X.~Yao, ``DDD: A new ensemble approach for dealing with concept drift,'' \textit{IEEE Transactions on Knowledge and Data Engineering}, vol.~24, no.~4, pp.~619--633, 2011.

\bibitem{goncalves2014comparative}
P.~M.~Gon\c{c}alves~Jr, S.~G.~T.~de Carvalho~Santos, R.~S.~M.~Barros, and D.~C.~L.~Vieira, ``A comparative study on concept drift detectors,'' \textit{Expert Systems with Applications}, vol.~41, no.~18, pp.~8144--8156, 2014.

\bibitem{gomes2019streaming}
H.~M.~Gomes, J.~Read, A.~Bifet, J.~P.~Barddal, and J.~Gama, ``Machine learning for streaming data: State of the art, challenges, and opportunities,'' \textit{ACM SIGKDD Explorations Newsletter}, vol.~21, no.~2, pp.~6--22, 2019.

\bibitem{cano2022rose}
A.~Cano and B.~Krawczyk, ``ROSE: Robust online self-adjusting ensemble for continual learning on imbalanced drifting data streams,'' \textit{Machine Learning}, vol.~111, pp.~2561--2592, 2022.

\bibitem{miller1956magical}
G.~A.~Miller, ``The magical number seven, plus or minus two: Some limits on our capacity for processing information,'' \textit{Psychological Review}, vol.~63, no.~2, pp.~81--97, 1956.

\bibitem{bifet2009adaptive}
A.~Bifet and R.~Gavald\`{a}, ``Adaptive learning from evolving data streams,'' in \textit{International Symposium on Intelligent Data Analysis}, pp.~249--260, 2009.

\bibitem{chiu2025cdcms}
C.~Chiu and L.~L.~Minku, ``CDCMS.CIL: Concept drift handling based on clustering in model space for class-imbalanced data streams,'' in \textit{IEEE International Conference on Data Science and Advanced Analytics (DSAA)}, 2025.

\bibitem{demsar2006statistical}
J.~Dem\v{s}ar, ``Statistical comparisons of classifiers over multiple data sets,'' \textit{Journal of Machine Learning Research}, vol.~7, pp.~1--30, 2006.

\end{thebibliography}

\end{document}
